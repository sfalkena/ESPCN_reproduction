{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"ESPCN_reproduction.ipynb","provenance":[],"collapsed_sections":["lreAOdKntZx8"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0-final"}},"cells":[{"cell_type":"markdown","metadata":{},"source":["# Reproduction of 'Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network'\n","> This blog post describes the reproduction of the paper: 'Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network'. It explains the main point of the paper, tries to reproduce the results of table 1 of the paper and argues about it's reproducibility\n","\n","- toc: true- branch: master- badges: true\n","- comments: true\n","- author: Luuk Balkenende & Sieger Falkena & Luc Kloosterlaan\n","- categories: [fastpages, jupyter]"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MUe-sNyYEtXz"},"source":["---\n","\n","### Running the experiment on Google Colab\n","This notebook is running remotely on the Google Colab platform. Therefore, to save and access the trained model, we needed to mount the Google drive. We used the following code snippet to set up a local drive on our computer."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"EB5FxQuGFNMD","colab":{}},"source":["from torchvision import transforms\n","from google.colab import drive\n","from torch.utils import data\n","from PIL import Image\n","\n","import matplotlib.pyplot as plt\n","import PIL.Image as pil_image\n","import torch.nn as nn\n","import random as rnd\n","import torchvision\n","import numpy as np\n","import torch\n","import time\n","import cv2\n","import os"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4n_lwVSHyt6u","colab_type":"code","outputId":"4b8392ec-c544-4aa7-fe28-1bad7ad1d253","executionInfo":{"status":"ok","timestamp":1587361520971,"user_tz":-120,"elapsed":34122,"user":{"displayName":"Sieger Falkena","photoUrl":"","userId":"16977573620042866490"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["drive.mount('/content/gdrive')\n","path ='/content/gdrive/My Drive/deep_learning_group_7'\n","os.chdir(path)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BcDCkYFThjkw","colab_type":"code","outputId":"a0671059-9987-4f18-ac9f-a9c1d7105811","executionInfo":{"status":"ok","timestamp":1587361527320,"user_tz":-120,"elapsed":3939,"user":{"displayName":"Sieger Falkena","photoUrl":"","userId":"16977573620042866490"}},"colab":{"resources":{"http://localhost:8080/saved_images/Set14/sr_img_1.png":{"data":"CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K","ok":false,"headers":[["content-length","1449"],["content-type","text/html; charset=utf-8"]],"status":404,"status_text":""},"http://localhost:8080/saved_images/Set14/lr_img_1.png":{"data":"CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K","ok":false,"headers":[["content-length","1449"],["content-type","text/html; charset=utf-8"]],"status":404,"status_text":""},"http://localhost:8080/saved_images/Set14/lr_img_8.png":{"data":"CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K","ok":false,"headers":[["content-length","1449"],["content-type","text/html; charset=utf-8"]],"status":404,"status_text":""},"http://localhost:8080/saved_images/Set14/lr_img_10.png":{"data":"CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K","ok":false,"headers":[["content-length","1449"],["content-type","text/html; charset=utf-8"]],"status":404,"status_text":""},"http://localhost:8080/saved_images/Set14/sr_img_10.png":{"data":"CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K","ok":false,"headers":[["content-length","1449"],["content-type","text/html; charset=utf-8"]],"status":404,"status_text":""},"http://localhost:8080/saved_images/Set14/sr_img_8.png":{"data":"CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K","ok":false,"headers":[["content-length","1449"],["content-type","text/html; charset=utf-8"]],"status":404,"status_text":""}},"base_uri":"https://localhost:8080/","height":409}},"source":["from IPython.display import HTML\n","HTML(filename=\"interactive_image.html\")"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<!DOCTYPE html>\n<html>\n<head>\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n<style>\n* {box-sizing: border-box;}\n\n.img-comp-container {\n  position: relative;\n  height: 300px; /*should be the same height as the images*/\n}\n\n.img-comp-img {\n  position: absolute;\n  width: auto;\n  height: auto;\n  overflow:hidden;\n}\n\n.img-comp-img img {\n  display:block;\n  vertical-align:middle;\n}\n\n.img-comp-slider {\n  position: absolute;\n  z-index:9;\n  cursor: ew-resize;\n  /*set the appearance of the slider:*/\n  width: 40px;\n  height: 40px;\n  background-color: #2196F3;\n  opacity: 0.7;\n  border-radius: 50%;\n}\n\n.column {\n  float: center;\n  width: 33.33%;\n  padding: 5px;\n}\n\n/* Clear floats after image containers */\n.row::after {\n  content: \"\";\n  clear: both;\n  display: table;\n}\n\n</style>\n<script>\nfunction initComparisons() {\n  var x, i;\n  /*find all elements with an \"overlay\" class:*/\n  x = document.getElementsByClassName(\"img-comp-overlay\");\n  for (i = 0; i < x.length; i++) {\n    /*once for each \"overlay\" element:\n    pass the \"overlay\" element as a parameter when executing the compareImages function:*/\n    compareImages(x[i]);\n  }\n  function compareImages(img) {\n    var slider, img, clicked = 0, w, h;\n    /*get the width and height of the img element*/\n    w = img.offsetWidth;\n    h = img.offsetHeight;\n    /*set the width of the img element to 50%:*/\n    img.style.width = (w / 2) + \"px\";\n    /*create slider:*/\n    slider = document.createElement(\"DIV\");\n    slider.setAttribute(\"class\", \"img-comp-slider\");\n    /*insert slider*/\n    img.parentElement.insertBefore(slider, img);\n    /*position the slider in the middle:*/\n    slider.style.top = (h / 2) - (slider.offsetHeight / 2) + \"px\";\n    slider.style.left = (w / 2) - (slider.offsetWidth / 2) + \"px\";\n    /*execute a function when the mouse button is pressed:*/\n    slider.addEventListener(\"mousedown\", slideReady);\n    /*and another function when the mouse button is released:*/\n    window.addEventListener(\"mouseup\", slideFinish);\n    /*or touched (for touch screens:*/\n    slider.addEventListener(\"touchstart\", slideReady);\n    /*and released (for touch screens:*/\n    window.addEventListener(\"touchstop\", slideFinish);\n    function slideReady(e) {\n      /*prevent any other actions that may occur when moving over the image:*/\n      e.preventDefault();\n      /*the slider is now clicked and ready to move:*/\n      clicked = 1;\n      /*execute a function when the slider is moved:*/\n      window.addEventListener(\"mousemove\", slideMove);\n      window.addEventListener(\"touchmove\", slideMove);\n    }\n    function slideFinish() {\n      /*the slider is no longer clicked:*/\n      clicked = 0;\n    }\n    function slideMove(e) {\n      var pos;\n      /*if the slider is no longer clicked, exit this function:*/\n      if (clicked == 0) return false;\n      /*get the cursor's x position:*/\n      pos = getCursorPos(e)\n      /*prevent the slider from being positioned outside the image:*/\n      if (pos < 0) pos = 0;\n      if (pos > w) pos = w;\n      /*execute a function that will resize the overlay image according to the cursor:*/\n      slide(pos);\n    }\n    function getCursorPos(e) {\n      var a, x = 0;\n      e = e || window.event;\n      /*get the x positions of the image:*/\n      a = img.getBoundingClientRect();\n      /*calculate the cursor's x coordinate, relative to the image:*/\n      x = e.pageX - a.left;\n      /*consider any page scrolling:*/\n      x = x - window.pageXOffset;\n      return x;\n    }\n    function slide(x) {\n      /*resize the image:*/\n      img.style.width = x + \"px\";\n      /*position the slider:*/\n      slider.style.left = img.offsetWidth - (slider.offsetWidth / 2) + \"px\";\n    }\n  }\n}\n</script>\n</head>\n<body>\n\n<h1>Comparison of low resolution and our upscaled image</h1>\n\n<p>Click and slide the blue slider to compare the two images:</p>\n\n<div class= \"row\">\n    <div class=\"img-comp-container\">\n      <div class=\"img-comp-img\">\n        <img src=\"espcn_reproduction_img/sr_img_1.png\" width=\"450\" height=\"300\">\n      </div>\n      <div class=\"img-comp-img img-comp-overlay\">\n        <img src=\"espcn_reproduction_img/lr_img_1.png\" width=\"450\" height=\"300\">\n      </div>\n    </div>\n    <div class=\"img-comp-container\">\n      <div class=\"img-comp-img\">\n        <img src=\"espcn_reproduction_img/sr_img_8.png\" width=\"450\" height=\"300\">\n      </div>\n      <div class=\"img-comp-img img-comp-overlay\">\n        <img src=\"espcn_reproduction_img/lr_img_8.png\" width=\"450\" height=\"300\">\n      </div>\n    </div>\n</div>\n\n\n<script>\n/*Execute a function that will execute an image compare function for each element with the img-comp-overlay class:*/\ninitComparisons();\n</script>\n\n</body>\n</html>"},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cM9ut5Utqedc"},"source":["# ESPCN Architecture\n","\n","Below, you can find the ESPCN architecture as defined in the paper. Notice that we have used `tanh` as activation function, as the writers indicated that this leads to better results. \n","\n","Most important to notice here is that the sub-pixel convolution layer is divided into two layers: one normal convolutional layer (`conv3`) and a layer which preforms the SR operation (`upsample`)."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lmSYnMfYGtlJ","colab":{}},"source":["r = 3 #scaling factor\n","\n","class SuperResConvNet(nn.Module):\n","    def __init__(self):\n","        super(SuperResConvNet, self).__init__()\n","        self.conv1 = nn.Conv2d(1,64, kernel_size=5, stride=1, padding=2)\n","        self.conv2 = nn.Conv2d(64,32, kernel_size=3, stride=1, padding=1)\n","        self.conv3 = nn.Conv2d(32,1*r**2, kernel_size=3, stride=1, padding=1)\n","        self.upsample = nn.PixelShuffle(r)\n","\n","    def forward(self, y):\n","        y = torch.tanh(self.conv1(y))\n","        y = torch.tanh(self.conv2(y))\n","        y = self.conv3(y)\n","        y = self.upsample(y)\n","        return y\n","\n","if torch.cuda.is_available():\n","    print(\"Using GPU\")\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","ConvNet = SuperResConvNet()\n","ConvNet.to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y-3ucPJyhHbE","colab_type":"text"},"source":["#Functions"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"P0boumoCMCIC","colab":{}},"source":["slide_subfolders = ['/CVPR2016_ESPCN_OurBenchMarkResult/Ours/yang91/T91/', \n","                    '/CVPR2016_ESPCN_OurBenchMarkResult/Ours/x3.0/Set5/', \n","                    '/CVPR2016_ESPCN_OurBenchMarkResult/Ours/x3.0/Set14/',\n","                    '/CVPR2016_ESPCN_OurBenchMarkResult/Ours/x3.0/BSD300/',\n","                    '/CVPR2016_ESPCN_OurBenchMarkResult/Ours/x3.0/BSD500/',\n","                    '/CVPR2016_ESPCN_OurBenchMarkResult/Ours/x3.0/SuperText136/']\n","\n","#width and height of patches\n","x = 17      \n","\n","class Slide:\n","    def __init__(self, path, slide_subfolders, count):\n","        self.count = count\n","        self.dir = path + slide_subfolders\n","        \n","        self.namelist = self.deleteLRImage()\n","        if count == 0:\n","            self.patchlist = self.patchlist_get()\n","        \n","    def removePng(f):\n","        'returns filename without png'\n","        filename_parts = f[:-4]\n","        return filename_parts\n","\n","    def getList(self):\n","        'returns list of filenames' \n","        return [Slide.removePng(f) for f in os.listdir(self.dir) if f.endswith(\".png\")]\n","\n","    def deleteLRImage(self):\n","        'deletes LR images from list if they are already made (which they are)'\n","        name_list_2 = Slide.getList(self)\n","        name_list_1 = [x for x in name_list_2 if \"lr\" not in x ]\n","        name_list =  [x for x in name_list_1 if \"lowRes\" not in x ]\n","        return name_list\n","    \n","    def getPatchList(self, img_name):\n","        'returns patch list of one image'\n","        #get filenames and load images\n","        filename = self.dir + img_name\n","        lr_img = cv2.cvtColor(cv2.imread(filename+'_lr.png'), cv2.COLOR_BGR2RGB)\n","\n","        #parameters \n","        stride_lr = x-np.sum((5%2,3%2,3%2))\n","        tot_img_d = int(lr_img.shape[0]/stride_lr), int(lr_img.shape[1]/stride_lr)    #amount of image in height and width respectively\n","        tot_img = tot_img_d[0]*tot_img_d[1]     #total amount of images\n","\n","        #create list for current image\n","        patch_list = []\n","        for i in range(tot_img_d[0]-1):\n","            for j in range(tot_img_d[1]-1):\n","                patch_list.append([img_name, i,j])\n","        return patch_list\n","    \n","    def patchlist_get(self):\n","        #create patch_list\n","        patch_list = []\n","        for i in range(len(self.namelist)):\n","            patch_list.extend(self.getPatchList(self.namelist[i]))\n","        print('Found', len(patch_list), 'trainable patches out of', len(self.namelist), 'images.')\n","        return patch_list\n","    \n","    def createPatch(self, name):\n","        'returns a patch'\n","        img_name = name[0]\n","        patch_name = name[1], name[2]\n","\n","        #get corresponding images\n","        filename = self.dir + img_name\n","        hr_img = cv2.cvtColor(cv2.imread(filename+'.png'), cv2.COLOR_BGR2YCrCb)[:,:,0] # only get Y channel from YCrCb\n","        lr_img = cv2.cvtColor(cv2.imread(filename+'_lr.png'), cv2.COLOR_BGR2YCR_CB)[:,:,0]\n","\n","        #create hr patch \n","        stride_hr = (x-np.sum((5%2,3%2,3%2)))*r\n","        hr_patch = hr_img[stride_hr*patch_name[0]:(stride_hr*patch_name[0]+17*r),stride_hr*patch_name[1]:(stride_hr*patch_name[1]+17*r)]\n","\n","        #create lr patch \n","        stride_lr = x-np.sum((5%2,3%2,3%2))\n","        lr_patch = lr_img[stride_lr*patch_name[0]:(stride_lr*patch_name[0]+17),stride_lr*patch_name[1]:(stride_lr*patch_name[1]+17)]\n","\n","        return lr_patch, hr_patch\n","    \n","    def image_operations_testing(self, img_n):\n","\n","        #get corresponding images\n","        hr_img = cv2.cvtColor(cv2.imread(self.dir + img_n + '.png'), cv2.COLOR_BGR2YCrCb)[:,:,0]\n","        lr_img = cv2.cvtColor(cv2.imread(self.dir + img_n[:-9] + '-lowRes.png'), cv2.COLOR_BGR2YCrCb)[:,:,0]\n","\n","        #difference in shape: lr*3 and hr, due to sub-sampling (few pixels difference)\n","        #--> reshape lr\n","        lr_img = lr_img[:-1,:-1]\n","        hr_img = hr_img[:lr_img.shape[0]*r,:lr_img.shape[1]*r]\n","\n","        return lr_img, hr_img\n","\n","    def colored_prediction(self, img_n):\n","      'creates colored prediction'\n","      #get corresponding images\n","      filename = self.dir + img_n\n","      hr_img = Image.open(filename + '.png').convert('YCbCr')\n","      lr_img = Image.open(filename[:-9] + '-lowRes.png').convert('YCbCr')\n","\n","      #upsample\n","      bicubic = lr_img.resize(hr_img.size, Image.BICUBIC)\n","\n","      return lr_img, bicubic, hr_img # return as pil images\n","\n","def getSlideList(slide_subfolders, path):\n","    slides = []\n","    for i, slide in enumerate(slide_subfolders):\n","        slides.append(Slide(path, slide, i))\n","    return slides\n","\n","slides = getSlideList(slide_subfolders, path)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lreAOdKntZx8"},"source":["### PSNR calculation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xq-fvSjktYfs","colab":{}},"source":["def psnr_from_mselist(mse_list):\n","  mse = np.mean(mse_list)\n","  if mse == 0:\n","    return float('inf')\n","  else:\n","    return 20*np.log10(255/np.sqrt(mse))\n","\n","def calc_mse(img_pred, img_hr):\n","    return np.mean((img_pred*255 - img_hr*255)**2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"O7S9BxrbFOhk"},"source":["# Obtaining low resolution images"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zMct6mkQlnwD","colab":{}},"source":["# ## DONT RUN THIS CELL, LR IMAGES ARE ALREADY MADE AND PRESENT IN DIRECTORY\n","# def createLowRes(img_name, dir_91):\n","#   'saves low resolution image'\n","#   # Call HR image\n","#   filename = dir_91 + img_name + '.png'\n","#   hr_img = cv2.cvtColor(cv2.imread(filename), cv2.COLOR_BGR2RGB)\n","\n","#   # Blur HR image\n","#   blur_img = cv2.GaussianBlur(hr_img,(5,5),0)\n","\n","#   # Apply subsampling\n","#   lr_img = blur_img[::r,::r]\n","#   #print('Shape HR image: ', hr_img.shape)\n","#   #print('Shape LR image: ', lr_img.shape)\n","\n","#   # Save lr_img\n","#   im = Image.fromarray(lr_img)\n","#   im.save(dir_91 + img_name + '_lr.png')\n","\n","# for i in range(len(slides[0].)):\n","#   dir_91 = path + slide_subfolders[0]\n","#   createLowRes(slides[0].namelist[i], dir_91)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"67c_d_O2p2aw"},"source":["---\n","# Training\n","-loss \\\n","-optimizer\n","\n","-stopping criterion (anders gedaan dan in paper) \\\n","-learning rate verhaal \\\n","\n","Eigenlijk zouden we hier nog validation bij moeten doen. Dus ongeveer 80% van patches gebruiken voor training, 20% gebruiken voor validation. Maar dat heb ik nu niet toegevoegd. Zouden we nog kunnen doen na morgenochtend"]},{"cell_type":"markdown","metadata":{"id":"7NeDugNcgrwV","colab_type":"text"},"source":["##Hyperparameter settings"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"gcBec_ZGpwSE","colab":{}},"source":["## Loss and optimizer\n","criterion = nn.MSELoss()     \n","optimizer = torch.optim.Adam(ConvNet.parameters(), lr=0.01)  \n","\n","## Stopping criterion\n","num_epoch = 5000     #amount of epochs\n","\n","## Scheduler for dynamic reduction of the learning rate\n","threshold_mu = 1e-6     # Treshold for decreasing learning rate. Default threshold: 0.0001, paper does not say anything about value of threshold.\n","factor_value = 0.8     # Amount of decay per step, new lr = factor_value*lr. Default value: 0.1, paper does not say anything about this value.\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=factor_value, patience=2, threshold=threshold_mu, min_lr=0.0001, eps=1e-08, verbose=True)\n","\n","## Batch size\n","batch_size = 16\n","\n","## training validation ratio\n","train_val_ratio = 0.95"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cn9sTJfSZU0t"},"source":["--- \n","### Data loader\n","-waarom en hoe we data loader/generator hebben gebruikt"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XoHMVNvCZw9-","colab":{}},"source":["class DataGenerator(data.Dataset):\n","    'Generates the dataset that is used for training the ESPCN'\n","    def __init__(self, slides):\n","        self.slides = slides\n","        self.transform = torchvision.transforms.Compose([\n","            torchvision.transforms.ToTensor()])\n","\n","    def __len__(self):\n","        return len(self.slides.patchlist)\n","\n","    def __getitem__(self, idx):\n","        lr_patch, hr_patch = self.slides.createPatch(self.slides.patchlist[idx])\n","        \n","        # transform images to pytorch tensors\n","        lr_patch = self.transform(lr_patch)\n","        hr_patch = self.transform(hr_patch)\n","\n","        return lr_patch, hr_patch   \n","\n","# Put DataGenerator in DataLoader\n","full_dataset = DataGenerator(slides[0])\n","\n","# Split between training and validation set\n","train_size = int(train_val_ratio * len(full_dataset))\n","validation_size = len(full_dataset) - train_size\n","training_set, validation_set = torch.utils.data.random_split(full_dataset, [train_size, validation_size])\n","\n","# num_workers should parallelize loading of the batches by the CPU\n","training_generator      = data.DataLoader(training_set, batch_size=batch_size, num_workers=batch_size, shuffle='True')\n","validation_generator    = data.DataLoader(validation_set, batch_size=1, num_workers=1, shuffle='False')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NDRaaKFtkZfr","colab_type":"text"},"source":["### Training the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Acbybp4KqT9j","colab":{}},"source":["# Lists used for saving losses and PSNR\n","loss_list = []\n","epoch_loss_list = []\n","val_loss_list = []\n","validation_psnr = []\n","\n","# Start stopwatch\n","t0 = time.time()\n","\n","for epoch in range(num_epoch):\n","    # Switch to training mode\n","    ConvNet.train()\n","    for i, (lr_patch, hr_patch) in enumerate(training_generator):\n","        \n","        # Transfer training data to active device\n","        lr_patch, hr_patch = lr_patch.to(device), hr_patch.to(device)\n","        \n","        # Run the forward pass\n","        outputs = ConvNet(lr_patch)\n","        loss = criterion(outputs, hr_patch)\n","        loss_list.append(loss.item())\n","\n","        # Backprop and perform Adam optimisation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Save loss every epoch\n","    epoch_loss = np.sum(loss_list)/len(training_generator)\n","    epoch_loss_list.append(epoch_loss)\n","    if epoch % 50 == 0:\n","        print(\"Epoch\", epoch, \"loss: {}\".format(epoch_loss))\n","    \n","    # Save model every 1000 epochs\n","    if epoch % 1000 == 999:\n","        'Save model every 1000 epochs (in case Google Colab stops runtime)'\n","        model_name = 'test3_mu_' + str(threshold_mu) + '_' + str(epoch+1) + '_epochs'\n","        path_model = path+'/introductory_notebooks/saved_models/' + model_name\n","        torch.save(ConvNet.state_dict(), path_model)\n","        print('Model saved as: ', model_name)\n","\n","    # Step to next step of lr-scheduler\n","    scheduler.step(epoch_loss)\n","    loss_list = []\n","\n","    # Enter validation mode\n","    ConvNet.eval()\n","    # Keep track of mse for every patch, to collectively calculate PSNR per epoch\n","    epoch_mse = []\n","    with torch.no_grad():\n","        for i, (lr_patch, hr_patch) in enumerate(validation_generator):\n","            # Transfer training data to active device (GPU)\n","            lr_patch, hr_patch = lr_patch.to(device), hr_patch.to(device)\n","\n","            # Predict output\n","            img_pred = ConvNet(lr_patch)\n","            \n","            # Calculate validation loss\n","            loss = criterion(img_pred, hr_patch)\n","            loss_list.append(loss.item())\n","            \n","            # Calculate mse for every sample \n","            img_pred = img_pred[0].cpu().numpy()\n","            hr_patch = hr_patch[0].cpu().numpy()\n","            epoch_mse.append(calc_mse(img_pred, hr_patch))\n","\n","        # Calculate validation psnr on the complete epoch from all individual MSE's\n","        val_psnr = psnr_from_mselist(np.array(epoch_mse))\n","        validation_psnr.append(val_psnr)\n","    \n","    # Save validation loss every epoch\n","    val_loss = np.sum(loss_list)/len(validation_generator)\n","    val_loss_list.append(val_loss)\n","    \n","    loss_list = []\n","\n","print('Training took {} seconds'.format(time.time() - t0))\n","print('Seconds per epoch:',(time.time()-t0)/num_epoch)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Ya3uJQhkfBJ","colab_type":"text"},"source":["### Plot training results"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"r6UHT2xf_2A6","colab":{}},"source":["# Show training and validation loss of current model in memory\n","plt.figure(figsize=(8,8))\n","plt.subplot(2,1,2)\n","plt.title('PSNR for validation data')\n","plt.plot(np.arange(num_epoch), validation_psnr)\n","\n","plt.subplot(2,1,1)\n","plt.title('Loss per epoch')\n","plt.plot(np.arange(num_epoch), epoch_loss_list, label='Training loss')\n","plt.plot(np.arange(num_epoch), val_loss_list, label='Validation loss')\n","plt.yscale(\"log\")\n","plt.legend()\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fW60iF7AknL5","colab_type":"text"},"source":["###Saving the model"]},{"cell_type":"code","metadata":{"id":"YNwQQC1CR3NT","colab_type":"code","colab":{}},"source":["# Save the current model in memory\n","model_name = path + '/introductory_notebooks/mu/' + 'mu_1e-2'\n","path_name = model_name + '_validation_psnr'\n","np.save(path_name, validation_psnr)\n","path_name = model_name + '_epoch_loss_list'\n","np.save(path_name, epoch_loss_list)\n","path_name = model_name + '_val_loss_list'\n","np.save(path_name, val_loss_list)\n","path_model = model_name + '_model'\n","torch.save(ConvNet.state_dict(), path_model)\n","print('Model saved as: ', model_name)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CKboRpOesOnI"},"source":["# Testing\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CVp0obPJDdL9"},"source":["#### Loading the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"odfKMQPuDdUU","colab":{}},"source":["model_name = 'final_test_5000_epochsweights'\n","path_model = path + '/introductory_notebooks/combined/' + model_name\n","ConvNet.load_state_dict(torch.load(path_model))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8rZWqi7Tllsu","colab_type":"text"},"source":["###Test image functions"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jfdJqCxnG6Dj","colab":{}},"source":["def generate_figure(lr_img, sr_img, hr_img):\n","  \"\"\"Show the low resolution image, together with the original high resolution image and upscaled version.\n","  Is used for a visual representation of how well the model behaves. \"\"\"\n","  f = plt.figure(figsize=(8*3,8))\n","  f.add_subplot(1, 3, 1)\n","  plt.imshow(transforms.ToPILImage('YCbCr')(lr_img[0]).convert('RGB'))\n","  f.add_subplot(1, 3, 2)\n","  plt.imshow(transforms.ToPILImage('YCbCr')(sr_img[0]).convert('RGB'))\n","  f.add_subplot(1, 3, 3)\n","  plt.imshow(transforms.ToPILImage('YCbCr')(hr_img[0]).convert('RGB'))\n","  \n","\n","def save_tensor_as_image(lr_img, sr_img, hr_img, i):\n","  \"\"\"Save the tensors as PIL images in the saved_image folder.\"\"\"\n","  transforms.ToPILImage('YCbCr')(lr_img[0]).convert('RGB').save(path + '/introductory_notebooks/saved_images/lr_img_' + str(i) + '.png')\n","  transforms.ToPILImage('YCbCr')(sr_img[0]).convert('RGB').save(path + '/introductory_notebooks/saved_images/sr_img_' + str(i) + '.png')\n","  transforms.ToPILImage('YCbCr')(hr_img[0]).convert('RGB').save(path + '/introductory_notebooks/saved_images/hr_img_' + str(i) + '.png')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8ID93xmdDTf8"},"source":["####Data loader"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"AuX2RB-qDshU","colab":{}},"source":["class DataGenerator_test(data.Dataset):\n","  'Generates the dataset used for testing'\n","\n","  def __init__(self, slides):\n","    self.slides = slides\n","    self.transform = torchvision.transforms.Compose([\n","            torchvision.transforms.ToTensor()])\n","\n","  def __len__(self):\n","    return len(self.slides.namelist)\n","\n","  def __getitem__(self, idx):\n","    lr_img, bicubic, hr_img = self.slides.colored_prediction(self.slides.namelist[idx])\n","    \n","    lr_img = self.transform(lr_img)\n","    hr_img = self.transform(hr_img)\n","    bicubic = self.transform(bicubic)\n","\n","\n","    return lr_img, bicubic, hr_img\n","\n","## Put DataGenerator in DataLoader\n","def DataGenerator(slides):\n","  test_set = DataGenerator_test(slides)\n","  test_generator = data.DataLoader(test_set, batch_size=1, shuffle=False)\n","  return test_generator"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mf0k-NdFmniU","colab_type":"text"},"source":["###Testing the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pBZ_7UqRDztA","colab":{}},"source":["# Create predictions\n","#0: Yang91.\n","#1: Set5\n","#2: Set14\n","#3: BSD300\n","#4: BSD500\n","#5: SuperText136\n","#To train on all datasets: for j in range(len(slides[1:]))\n","\n","rnd.seed(4)\n","testsets = ['Set5','Set14','BSD300','BSD500','SuperText136']\n","\n","for j in range(len(slides[1:])):\n","  test_generator = DataGenerator(slides[j+1])\n","  show = rnd.randint(0,len(test_generator))\n","\n","  start_time = time.time()\n","  outputs = []\n","  mse_list = []\n","  #for epoch in range(1): \n","  for i, (lr_img, bicubic, hr_img) in enumerate(test_generator): #currently in form: [1,3,x,y]\n","    with torch.no_grad():\n","        \n","      # Only test on the Y (intensity channel)\n","      to_network = (lr_img[:,0,:,:]).unsqueeze(0)\n","\n","      #Run the forward pass. Testing is done on CPU now\n","      outputs = ConvNet.cpu()(to_network)\n","      img_pred = outputs[0]\n","\n","      # substitute the Y channel from bicubic with the one outputted by the model\n","      bicubic[:,0,:,:] = img_pred \n","    \n","      mse = calc_mse(img_pred.numpy(), hr_img.numpy()[:,0,:,:])\n","      mse_list.append(mse.item())\n","\n","      if i == show:\n","        generate_figure(lr_img, bicubic, hr_img)\n","        save_tensor_as_image(lr_img, bicubic, hr_img, i)\n","\n","  elapsed_time = time.time() - start_time\n","  print(testsets[j], ': ', psnr_from_mselist(mse_list))"],"execution_count":0,"outputs":[]}]}