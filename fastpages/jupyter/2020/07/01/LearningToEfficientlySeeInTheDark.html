<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Learning To Efficiently See In The Dark | Sieger Falkena</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Learning To Efficiently See In The Dark" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This article describes the binarization of the network in the paper ‘Learning to See in the Dark’ [1] using the approach of the ABC-net presented in ‘Towards Accurate Binary Convolutional Neural Network’ [2]" />
<meta property="og:description" content="This article describes the binarization of the network in the paper ‘Learning to See in the Dark’ [1] using the approach of the ABC-net presented in ‘Towards Accurate Binary Convolutional Neural Network’ [2]" />
<link rel="canonical" href="https://sfalkena.github.io/blogs/fastpages/jupyter/2020/07/01/LearningToEfficientlySeeInTheDark.html" />
<meta property="og:url" content="https://sfalkena.github.io/blogs/fastpages/jupyter/2020/07/01/LearningToEfficientlySeeInTheDark.html" />
<meta property="og:site_name" content="Sieger Falkena" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-07-01T00:00:00-05:00" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://sfalkena.github.io/blogs/fastpages/jupyter/2020/07/01/LearningToEfficientlySeeInTheDark.html"},"description":"This article describes the binarization of the network in the paper ‘Learning to See in the Dark’ [1] using the approach of the ABC-net presented in ‘Towards Accurate Binary Convolutional Neural Network’ [2]","@type":"BlogPosting","url":"https://sfalkena.github.io/blogs/fastpages/jupyter/2020/07/01/LearningToEfficientlySeeInTheDark.html","headline":"Learning To Efficiently See In The Dark","dateModified":"2020-07-01T00:00:00-05:00","datePublished":"2020-07-01T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blogs/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://sfalkena.github.io/blogs/feed.xml" title="Sieger Falkena" /><link rel="shortcut icon" type="image/x-icon" href="/blogs/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Learning To Efficiently See In The Dark | Sieger Falkena</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Learning To Efficiently See In The Dark" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This article describes the binarization of the network in the paper ‘Learning to See in the Dark’ [1] using the approach of the ABC-net presented in ‘Towards Accurate Binary Convolutional Neural Network’ [2]" />
<meta property="og:description" content="This article describes the binarization of the network in the paper ‘Learning to See in the Dark’ [1] using the approach of the ABC-net presented in ‘Towards Accurate Binary Convolutional Neural Network’ [2]" />
<link rel="canonical" href="https://sfalkena.github.io/blogs/fastpages/jupyter/2020/07/01/LearningToEfficientlySeeInTheDark.html" />
<meta property="og:url" content="https://sfalkena.github.io/blogs/fastpages/jupyter/2020/07/01/LearningToEfficientlySeeInTheDark.html" />
<meta property="og:site_name" content="Sieger Falkena" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-07-01T00:00:00-05:00" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://sfalkena.github.io/blogs/fastpages/jupyter/2020/07/01/LearningToEfficientlySeeInTheDark.html"},"description":"This article describes the binarization of the network in the paper ‘Learning to See in the Dark’ [1] using the approach of the ABC-net presented in ‘Towards Accurate Binary Convolutional Neural Network’ [2]","@type":"BlogPosting","url":"https://sfalkena.github.io/blogs/fastpages/jupyter/2020/07/01/LearningToEfficientlySeeInTheDark.html","headline":"Learning To Efficiently See In The Dark","dateModified":"2020-07-01T00:00:00-05:00","datePublished":"2020-07-01T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://sfalkena.github.io/blogs/feed.xml" title="Sieger Falkena" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blogs/">Sieger Falkena</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blogs/about/">About Me</a><a class="page-link" href="/blogs/search/">Search</a><a class="page-link" href="/blogs/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Learning To Efficiently See In The Dark</h1><p class="page-description">This article describes the binarization of the network in the paper &#39;Learning to See in the Dark&#39; [1] using the approach of the ABC-net presented in &#39;Towards Accurate Binary Convolutional Neural Network&#39; [2]</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-07-01T00:00:00-05:00" itemprop="datePublished">
        Jul 1, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      25 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blogs/categories/#fastpages">fastpages</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blogs/categories/#jupyter">jupyter</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/sfalkena/blogs/tree/master/_notebooks/2020-06-31-LearningToEfficientlySeeInTheDark.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/blogs/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/sfalkena/blogs/master?filepath=_notebooks%2F2020-06-31-LearningToEfficientlySeeInTheDark.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blogs/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/sfalkena/blogs/blob/master/_notebooks/2020-06-31-LearningToEfficientlySeeInTheDark.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blogs/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-06-31-LearningToEfficientlySeeInTheDark.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="By-Sieger-Falkena,-Anwesh-Marwade-and-Joris-Quist">By Sieger Falkena, Anwesh Marwade and Joris Quist<a class="anchor-link" href="#By-Sieger-Falkena,-Anwesh-Marwade-and-Joris-Quist"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>With the ever-growing amount of papers published, the field of Computer Vision by Deep Learning is more relevant than ever. Many new solutions are presented every day. Models get bigger and more complex. Together with computer vision, the market of smart-phone and smart-wearables is growing. Estimates have been made that in 2020, around 1.4 trillion photo's will be made worldwide from which 90.9% will originate from smartphones [4].</p>
<p>However, the differences in computing power between the two sectors is enormous. Deep learning models require powerful GPU's to train and evaluate a model. Smartphones don't offer this. To accommodate that the two sectors are growing towards each other, two paths are being followed: smartphones are equipped with more powerful hardware every year making that smartphones can offer more services. On the other hand, deep learning models are made more and more efficient, so that evaluation can be done on a wearable device.</p>
<p>In this article, we want to take a step towards the direction of efficient neural networks. We will combine methods from different active fields of study and present an efficient method to process low-light images. We showcase the implementation details of applying the concept of network binarization using "Accurate Binary Convolutional Network" (ABC-Net) in "Learning To See In The Dark" (LTSITD) paradigm. In doing so, we want to find out whether the full accuracy network can be approximated by binarization. We aim for limited performance loss as compared to the full precision model while driving up the efficiency. We intend to highlight the theoretical speedup that can be achieved and shed light on the limitations that we faced.</p>
<p><strong>Click <a href="https://sfalkena.github.io/blogs/_pages/interactive_image.html">https://sfalkena.github.io/blogs/_pages/interactive_image.html</a> or scan the QR code below to see an interactive comparison of the results in high resolution! Warning: the images are in high resolution, so the loading of images might be slow ;)</strong></p>
<p><img src="https://docs.google.com/u/0/uc?id=1A48SXJfG5UdbfFCoiEHqS_ET28XXrDrR" alt="alt text" /></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Techniques-used">Techniques used<a class="anchor-link" href="#Techniques-used"> </a></h1><p>First, let us introduce the problem that we propose to tackle. This problem is presented in [1], where the authors take a raw image file (short-exposure images which are grainy and suffer from noise due to low-lighting) and use a deep neural network to learn the image-processing pipeline to process the low-light raw data. By working directly with raw sensor data, the authors intend to replace the standard imaging pipeline. Intending to achieve this, the authors train a convolutional network with a new dataset involving raw short-exposure low-light images with corresponding long-exposure images as the target reference or ground truth.</p>
<p>The pipeline suggested by the authors can be seen in the figure below. First, the raw image is unpacked, the black levels are subtracted after which the image is amplified. Finally, the image is fed through the CNN, and the output is an RGB image.</p>
<p><img src="https://docs.google.com/u/0/uc?id=1R5ppuIzbFaGlVnhlgeeCvs6Wk59zTN9_" alt="" /></p>
<p>Figure \ref{abc_arch}</p>
<p>For examples of the results achieved by [1], look at <a href="https://cchen156.github.io/SID.html">https://cchen156.github.io/SID.html</a> for an interactive comparison.</p>
<p>To make the ConvNet in the figure shown above more efficient, binarization is applied to the ConvNet. For this, we follow the fundamental techniques of binary convolution using XNOR and bitcount operations from [5]. In applying binarization, we explore two schemes for implementing a binary network namely, XNOR-Net and ABC-Net. In the following section, we first explain the XNOR-Net, after which we dive into the ABC-Net implementation in detail.</p>
<p>For the interested reader, we have written a chapter where we explain the basics of <a href="https://drive.google.com/file/d/1BnX25RuH6bDnD2rfNdJAfkTVJhA4irEl/view?usp=sharing">Binary Networks</a> in more detail.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="XNOR-Net">XNOR-Net<a class="anchor-link" href="#XNOR-Net"> </a></h2><p>The first binarization technique that we explored is the XNOR-Net implementation presented in [5]. In XNOR-Networks, both the filters and the input to the convolutional layers are binarized which allows these networks to approximate the convolutions primarily using binary operations. The efficiency and speedup can be achieved by leveraging the XNOR and bit-counting operations to estimate the convolutions.</p>
<p>We replace the LSTITD layers with the binarized convolutions which are mentioned in detail in the ABC-Net section. In this effort, even though we observed an increased efficiency in the network (in terms of memory and speed-up), 
reading further into the implementation we realised that XNOR-Net is a specialization of the ABC-Net. Being a generalisation, the ABC-net extends better representational power by approximating the convolutions using multiple binarized weights and activations. Additionally, lack of a low-level library that can efficiently perform the bit-counting operations was not very ideal and is discussed in detail in the section on technical constraints.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="ABC-Net">ABC-Net<a class="anchor-link" href="#ABC-Net"> </a></h2><p>We implemented this by using the follow-up work shown in [2] to achieve higher accuracy than XNOR nets in [5]. The technique used in [2] is called Accurate Binary Convolutional Network or ABC-Net. Explained very briefly, ABC-Net approximates the full precision convolution using $M$ binary convolutions (XNOR-Nets). The binary convolutions (BinConvs) are implemented using the technique from [5].</p>
<p>For the interested reader, we have written a chapter where we explain the basics of <a href="https://drive.google.com/file/d/1BnX25RuH6bDnD2rfNdJAfkTVJhA4irEl/view?usp=sharing">Binary Networks</a> in more detail. However, we intend to provide some intuition regarding the ABC-Net as mentioned below.</p>
<p><img src="https://docs.google.com/u/0/uc?id=1TL_Ma1Hs-KYk1eSKY_P24Pc9sMyJhxDS" alt="abc_arch" /></p>
<p>The ABC-Net approximates each real-valued convolution by using multiple binary values per weight, giving it better representational power. The approximation is achieved as follows: 
\begin{equation}
W \approx \alpha_1\mathbf{B}_1 + \alpha_2\mathbf{B}_2 + ... + \alpha_M\mathbf{B}_M
\end{equation}</p>
<p>Finding a good value for $\mathbf{B}_i$ and $\alpha_i$ comes down to the following optimization problem:</p>
\begin{equation}
\min\limits_{\mathbf{\alpha}, \mathbf{B}}J(\mathbf{\alpha}, \mathbf{B}) = ||\mathbf{w} - \mathbf{B} \mathbf{\alpha}||^2,\;\;\;\; s.t.\:\mathbf{B}_{ij}\in \{-1,+1\}
\end{equation}<p>Where $\mathbf{B} = [vec(\mathbf{B}_1), vec(\mathbf{B}_2), ..., vec(\mathbf{B}_M)$, $\;\mathbf{w} = vec(\mathbf{W})$ and $\mathbf{\alpha} = [\alpha_1, \alpha_2, ..., \alpha_M]^T$. Trying to find the optimum to this problem numerically, makes it quite difficult as it is no longer possible to compute the derivative w.r.t. $\textbf{W}$ using the Straight-Through-Estimator [add reference]. Therefore the optimum is approximated by first calculating each $B_i$ in the following way:</p>
\begin{equation}
\mathbf{B}_i = sign(\overline{\textbf{W}} + u_istd(\mathbf{W})), \; i = 1,2,...,M 
\end{equation}<p>Where $\overline{\textbf{W}}$ are the normalized weights and $u_i$ is picked evenly over the the range $[-std(\mathbf{W}), std(\mathbf{W})]$. This is done because experimental observations show that real-valued weights often look like they are drawn from a Gaussian distribution.</p>
<p>Importantly, in order to take full advantage of the more efficient XNOR and bitcount operations for the convolutions, the activations need to be binarized in addition to the network weights. The ABC-Net does this by using multiple bits per activation to get a better approximation of the original i.e. full precision activations. However, it is not desirable to use the same method for binarizing the activations as we do for the weights. The reason being that the alpha values (and the beta values for the activations) need to be calculated using linear regression. For the weights, it is not a big problem, since the regression needs to happen only when the weights need to be updated which int turn happens only during training. Hence, it does not impact the inference, which should be as efficient as possible. However this is not the case for the activations since these always vary at test time.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Theoretical-speedup">Theoretical speedup<a class="anchor-link" href="#Theoretical-speedup"> </a></h2><p>Before we dive into the speedup that would theoretically be possible, we need to understand more about the factors that influence this speedup. The speedup is achieved by binarizing the weights and activations of the convolution. For binarizing the weights, the full precision weights are approximated with $\boldsymbol{W} \approx \alpha_{1} \boldsymbol{B}_{1}+\alpha_{2} \boldsymbol{B}_{2}+\cdots+\alpha_{M} \boldsymbol{B}_{M}$. So $M$ BinConvs are used as can be seen on the left part of the figure above. Furthermore, the activations $R$ can also be binarized by approximating $\boldsymbol{R} \approx \beta_{1} A_{1}+\beta_{2} A_{2}+\cdots+\beta_{N}\boldsymbol{A}_{N}$.</p>
<p>According to [5] the speedup of using XNOR-Net is the ratio of the operations carried out by a normal convolution and the binarized XNOR convolution: $S=\frac{64 c N_{W}}{c N_{W} + 64}$, where $N_{\mathbf{W}}=w h$ (the number of entries in the weights) and $c$ is the number of channels. For justification of this equation, please refer to the original paper. The alert reader will notice that the speedup does not depend on the size of the input.</p>
<p>Because there are now $N\times M$ of these BinConvs, the speedup per convolutional layer linearly decreases with this number and becomes $S=\frac{64 c N_{W}}{M N (c N_{W} + 64)}$. To draw conclusions about the total speedup of the network, we have to look into the architecture and take the average of all the convolutional layers with different amount of channels and inputs.</p>
<p>In the results section, the numbers of different architectures will be shown.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Technical-constraints">Technical constraints<a class="anchor-link" href="#Technical-constraints"> </a></h1><p>In the introduction, as an alert reader must have noticed, we stress that we are aiming for limiting the performance loss rather than only aiming for increase in efficiency. The reason for this is the fact that binarization is still a fairly new field of study. To convert the theoretical speedup of the network into practice, special hardware or software needs to be used. The speedup is realized by using XNOR and bitcount operations [1]. Intel researchers in [6]  have compared running a binarized network on different architectures, and they have pointed out that specialised hardware like FPGAs have a greater potential for running binarized networks. For now, the platform that we use i.e. PyTorch does not offer possibilities to achieve and leverage this practical speedup to which end we feel that it is currently out of the scope of this project.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Implementation-of-the-code">Implementation of the code<a class="anchor-link" href="#Implementation-of-the-code"> </a></h1><p>In this section, certain relevant parts of the code will be highlighted.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Architecture">Architecture<a class="anchor-link" href="#Architecture"> </a></h2><p>Below, the architecture of the binarized LTSITD (ABCLSID) is shown. Some remarks about the architecture:</p>
<ul>
<li>The first and last layer are not binarized and uses normal convolution. The speedup of binarizing is smaller here because of the low amount of channels or small kernel size. </li>
<li>Rest of architectural decisions (channels, kernel size, etc.) is kept the same as the original LTSITD. The layer order from LTSITD adhered to the layer order recommended by [5].</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">ABCLSID</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inchannel</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">binary_transposed_conv</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ABCLSID</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">M</span> <span class="o">=</span> <span class="n">M</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N</span> <span class="o">=</span> <span class="n">N</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">binary_transposed_conv</span> <span class="o">=</span> <span class="n">binary_transposed_conv</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">conv1_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">inchannel</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lrelu</span> <span class="o">=</span>  <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">negative_slope</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1_2</span> <span class="o">=</span> <span class="n">ABCConv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">M</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">conv2_1</span> <span class="o">=</span> <span class="n">ABCConv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">M</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2_2</span> <span class="o">=</span> <span class="n">ABCConv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">M</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">conv3_1</span> <span class="o">=</span> <span class="n">ABCConv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">M</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv3_2</span> <span class="o">=</span> <span class="n">ABCConv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">M</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">conv4_1</span> <span class="o">=</span> <span class="n">ABCConv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">M</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv4_2</span> <span class="o">=</span> <span class="n">ABCConv2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">M</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">conv5_1</span> <span class="o">=</span> <span class="n">ABCConv2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">M</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv5_2</span> <span class="o">=</span> <span class="n">ABCConv2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">M</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">binary_transposed_conv</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">up6</span> <span class="o">=</span> <span class="n">ABCConvTranspose2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">M</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">up6</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">conv6_1</span> <span class="o">=</span> <span class="n">ABCConv2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">M</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv6_2</span> <span class="o">=</span> <span class="n">ABCConv2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">M</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">binary_transposed_conv</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">up7</span> <span class="o">=</span> <span class="n">ABCConvTranspose2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">M</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">up7</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">conv7_1</span> <span class="o">=</span> <span class="n">ABCConv2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">M</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv7_2</span> <span class="o">=</span> <span class="n">ABCConv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">M</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">binary_transposed_conv</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">up8</span> <span class="o">=</span> <span class="n">ABCConvTranspose2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">M</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">up8</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">conv8_1</span> <span class="o">=</span> <span class="n">ABCConv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">M</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv8_2</span> <span class="o">=</span> <span class="n">ABCConv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">M</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">binary_transposed_conv</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">up9</span> <span class="o">=</span> <span class="n">ABCConvTranspose2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">M</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">up9</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">conv9_1</span> <span class="o">=</span> <span class="n">ABCConv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">M</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv9_2</span> <span class="o">=</span> <span class="n">ABCConv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">M</span><span class="p">)</span>

        <span class="n">out_channel</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv10</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">out_channel</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
                <span class="n">n</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">m</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">m</span><span class="o">.</span><span class="n">out_channels</span>
                <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span> <span class="o">/</span> <span class="n">n</span><span class="p">))</span>
                <span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">):</span>
                <span class="n">n</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">m</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">m</span><span class="o">.</span><span class="n">out_channels</span>
                <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span> <span class="o">/</span> <span class="n">n</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Datasets-and-data-processing">Datasets and data processing<a class="anchor-link" href="#Datasets-and-data-processing"> </a></h2><p>In the section "Techniques used", we already mentioned briefly that the authors of [1] created a new dataset to train and evaluate the model. In this section, we want to explain a bit more about the importance of the dataset.</p>
<p>Let's start with explaining what kind of data we have. The authors have created a dataset with low-light images. The original dataset has been shot with two cameras: A Sony $\alpha$7S II and a Fujifilm X-T2. The reason to use two different cameras has to do with the different ways the images are stored. The Sony uses a Bayer filter, where the Fujifilm uses a X-Trans filter. We leave the exploration of the X-Trans filter to the reader as we have chosen to focus on the Sony camera. The reason for this is that Sony is the leading company in the smartphone cameras holds <a href="https://www.bloomberg.com/news/articles/2019-12-23/sony-can-t-make-image-sensors-fast-enough-to-keep-up-with-demand">50.1\%</a> of the market share in 2019. As said, the Sony images use a Bayer filter. We would suggest reading <a href="https://www.cambridgeincolour.com/tutorials/camera-sensors.htm">this blog</a> first, to understand more about Bayer filters. The first image of our blog shows the input image being in Bayer form. The demosaicing step in the aforementioned blog is included in the ConvNet pipeline.</p>
<p>So, back to the dataset. The dataset contains groups of images shot in the same way, but with different exposure times, making that we have bright ground truth images, together with dark low-exposure images. To test the rigidity of the network, images with exposure times of $0.033s, 0.04s, 0.1s$ have been captured.</p>
<p>The raw images are of size 4256x2848, but this is still in Bayer form, so they are unpacked to a 2128x1424 images with 4 color channels. As these images are still very big to load into memory as a whole. So big, that it was not possible to perform backpropagation with the entire image as input, because then our GPU would run out of memory (8 GiB). That is why we train the network on patches of the images. The patch size used is 512x512 pixels.</p>
<p>Another problem was that training was very slow, at first it would have taken around 6 days to train the network. The reason for this was the way the images were loaded and processed. The way it was done, was that each iteration, an images was loaded from disk, some processing was done (by the python rawpy module) and then a random patch was taken from that image. The bottleneck at that point was the loading of the images from disk to memory. Even with 8 data loading workers that simultaneously loaded images it was not enough and using more workers caused overheads on other places so that also did not improve performance. The way the authors overcame this problem was keeping all the data in memory. The problem for us was however, that the dataset was 64 Gigabytes and we only had 16 Gigabytes of RAM. The way we fixed this was by preprocessing the data. We first loaded the images, then did the processing, but then instead of taking a random patch, we took 15 patches in a grid of 3 by 5. This was the smallest amount of patches that together (with a bit of overlap) covered the entire image. We then stored the patches. This way when training we could directly load the patch instead of the entire image. This improved our training speed significantly, so then we could train the network in only 22 hours.
This method does have some disadvantages, because this way we always use the same patches instead of a random ones, which could in theory affect the result after training. However we still got very comparable results on the test set as the original model, so it probably did not matter much.
Another problem is that the raw images had an 8-bit encoding, but after processing this gets converted to 32-bit tensors. Plus the patches were slightly overlapping. Together this caused our new processed training set to be 353GB, which is slightly less practical to store.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Binary-convolution">Binary convolution<a class="anchor-link" href="#Binary-convolution"> </a></h2><p>One problem we encountered during the project was because we did not have access to efficient implementations of the binary convolutions implying that we had to use the standard convolutions for 32-bit Floating-point numbers. This meant that instead of a getting a speedup as mentioned in Section "Theoretical speedup", it took about $M\cdot N$ times longer to do a forward pass, since that is the amount of convolutions we now had to do in each layer. This would mean that training would again take too long. To remedy this, we slightly deviated from the procedure that is described in the ABC-Net paper. We still calculated the binary tensors and their multipliers in the same way, but instead of using them in separate convolutions, we used them to approximate the original weights again. In theory this should give the same results. In practice, both these methods had small differences in their outputs. This is probably due to floating-point errors. These errors, however, were minute enough to not impact a real difference during training or in performance. In the code snippet below, we first have two functions implementing the shift operation as in equation 2 from [2].</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">import</span> <span class="nn">torch</span>


<span class="k">def</span> <span class="nf">shift_parameter_binarization</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">shift_parameters</span><span class="p">,</span> <span class="n">empty_cache</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">empty_cache</span><span class="p">:</span>
        <span class="c1"># print(&quot;Emptying cache&quot;)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
        <span class="p">[((</span><span class="n">tensor</span> <span class="o">-</span> <span class="n">tensor</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">+</span> <span class="n">shift_parameter</span><span class="p">)</span><span class="o">.</span><span class="n">sign</span><span class="p">()</span> <span class="k">for</span> <span class="n">shift_parameter</span> <span class="ow">in</span> <span class="n">shift_parameters</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">shift_parameter_binarization_activation</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">shift_parameters</span><span class="p">,</span> <span class="n">empty_cache</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">empty_cache</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
        <span class="p">[((</span><span class="n">tensor</span> <span class="o">-</span> <span class="n">tensor</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">+</span> <span class="n">shift_parameter</span><span class="p">)</span><span class="o">.</span><span class="n">sign</span><span class="p">()</span> <span class="k">for</span> <span class="n">shift_parameter</span> <span class="ow">in</span> <span class="n">shift_parameters</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Binary-2D-convolution">Binary 2D convolution<a class="anchor-link" href="#Binary-2D-convolution"> </a></h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">ABCConv2d</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">estimated_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">ABCConv2d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
                                        <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
                                        <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="n">padding_mode</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">M</span> <span class="o">=</span> <span class="n">M</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N</span> <span class="o">=</span> <span class="n">N</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">betas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">N</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">N</span><span class="p">))</span>

            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">betas</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vs</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">use_estimated_tensors</span> <span class="o">=</span> <span class="n">estimated_weights</span>

    <span class="c1"># Calculates the alphas based on the binarized and real weights using linear regression</span>
    <span class="k">def</span> <span class="nf">get_alphas</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">B</span><span class="p">):</span>
        <span class="n">vectorized_B</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">vectorized_B</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">vectorized_B</span><span class="p">)</span><span class="o">.</span><span class="n">inverse</span><span class="p">()</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">vectorized_B</span><span class="o">.</span><span class="n">t</span><span class="p">())</span><span class="o">.</span><span class="n">mv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Binarize the weights in M matrices</span>
    <span class="k">def</span> <span class="nf">get_B</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span>

        <span class="k">return</span> <span class="n">shift_parameter_binarization</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="p">[(</span><span class="o">-</span><span class="mi">1</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">M</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">weights</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span>
                                                      <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">)],</span> <span class="n">empty_cache</span><span class="o">=</span><span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">))</span>

    <span class="c1"># Binarize the weights in M matrices</span>
    <span class="k">def</span> <span class="nf">get_binary_activation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="n">binary_input</span> <span class="o">=</span> <span class="n">shift_parameter_binarization_activation</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span>
                                                               <span class="p">[(</span><span class="o">-</span><span class="mi">1</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="nb">input</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span>
                                                                <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">)],</span>
                                                               <span class="n">empty_cache</span><span class="o">=</span><span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">))</span>

        <span class="n">vectorized_binary_input</span> <span class="o">=</span> <span class="n">binary_input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
        <span class="n">betas</span> <span class="o">=</span> <span class="n">vectorized_binary_input</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">vectorized_binary_input</span><span class="p">)</span><span class="o">.</span><span class="n">pinverse</span><span class="p">()</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">vectorized_binary_input</span><span class="o">.</span><span class="n">t</span><span class="p">())</span><span class="o">.</span><span class="n">mv</span><span class="p">(</span>
                <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="n">estimated_binary_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mv</span><span class="p">(</span><span class="n">vectorized_binary_input</span><span class="p">,</span> <span class="n">betas</span><span class="p">)</span>
        <span class="c1"># del binary_input</span>
        <span class="k">return</span> <span class="n">estimated_binary_input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_estimated_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
        <span class="n">vectorized_B</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
        <span class="n">estimated_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mv</span><span class="p">(</span><span class="n">vectorized_B</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">estimated_weights</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># if self.training or self.B is None or self.alpha is None:</span>
        <span class="n">B</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_B</span><span class="p">()</span>
        <span class="n">alphas</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_alphas</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">:</span>
            <span class="n">x</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_binary_activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_estimated_tensors</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_estimated_weights</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">alphas</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                                     <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">alphas</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                                     <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                                                     <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
                                                     <span class="n">dilation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
                                                     <span class="n">groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">))</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">x</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Binary-2D-transposed-convolution">Binary 2D transposed convolution<a class="anchor-link" href="#Binary-2D-transposed-convolution"> </a></h3><p>For the transposed convolution, we have not yet seen any implementation online, so we have estimated that it can be done in a similar way as the normal binary 2D convolution.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">ABCConvTranspose2d</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">estimated_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">ABCConvTranspose2d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
                                                 <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
                                                 <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="n">padding_mode</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">M</span> <span class="o">=</span> <span class="n">M</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N</span> <span class="o">=</span> <span class="n">N</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">betas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">N</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">N</span><span class="p">))</span>

            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">betas</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vs</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">use_estimated_tensors</span> <span class="o">=</span> <span class="n">estimated_weights</span>

    <span class="c1"># Calculates the alphas based on the binarized and real weights using linear regression</span>
    <span class="k">def</span> <span class="nf">get_alphas</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">B</span><span class="p">):</span>
        <span class="n">vectorized_B</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">vectorized_B</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">vectorized_B</span><span class="p">)</span><span class="o">.</span><span class="n">pinverse</span><span class="p">()</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">vectorized_B</span><span class="o">.</span><span class="n">t</span><span class="p">())</span><span class="o">.</span><span class="n">mv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

    <span class="c1"># Binarize the weights in M matrices</span>
    <span class="k">def</span> <span class="nf">get_B</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span>

        <span class="k">return</span> <span class="n">shift_parameter_binarization</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="p">[(</span><span class="o">-</span><span class="mi">1</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">M</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">weights</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span>
                                                      <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">)],</span> <span class="n">empty_cache</span><span class="o">=</span><span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

    <span class="c1"># Binarize the weights in M matrices</span>
    <span class="k">def</span> <span class="nf">get_binary_activation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="n">input_data</span> <span class="o">=</span> <span class="nb">input</span>
        <span class="n">binary_input</span> <span class="o">=</span> <span class="n">shift_parameter_binarization_activation</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span>
                                                               <span class="p">[(</span><span class="o">-</span><span class="mi">1</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">input_data</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span>
                                                                <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">)],</span> <span class="n">empty_cache</span><span class="o">=</span><span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">))</span>

        <span class="n">vectorized_binary_input</span> <span class="o">=</span> <span class="n">binary_input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
        <span class="n">betas</span> <span class="o">=</span> <span class="n">vectorized_binary_input</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">vectorized_binary_input</span><span class="p">)</span><span class="o">.</span><span class="n">pinverse</span><span class="p">()</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">vectorized_binary_input</span><span class="o">.</span><span class="n">t</span><span class="p">())</span><span class="o">.</span><span class="n">mv</span><span class="p">(</span>
            <span class="n">input_data</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="n">estimated_binary_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mv</span><span class="p">(</span><span class="n">vectorized_binary_input</span><span class="p">,</span> <span class="n">betas</span><span class="p">)</span>
        <span class="k">del</span> <span class="n">binary_input</span>
        <span class="k">return</span> <span class="n">estimated_binary_input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">input_data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_estimated_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
        <span class="n">vectorized_B</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
        <span class="n">estimated_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mv</span><span class="p">(</span><span class="n">vectorized_B</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">estimated_weights</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_estimated_activations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_binarized_activations</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">betas</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">B</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_B</span><span class="p">()</span>
        <span class="n">alphas</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_alphas</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>

        <span class="c1"># TODO check best way to perform input normalization</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">:</span>
            <span class="n">x</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_binary_activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_estimated_tensors</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">conv_transpose2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_estimated_weights</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">alphas</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                                               <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">alphas</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">conv_transpose2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                                               <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                                                               <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
                                                               <span class="n">dilation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
                                                               <span class="n">groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">))</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">x</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Activation-Binarization">Activation Binarization<a class="anchor-link" href="#Activation-Binarization"> </a></h3><p>Since we couldn't get normalization to work and the binarization of the activations in ABC-Net depend on this, we needed to find a new way to do this. In the end we chose to do it the same way as with the binarization of the weights. By picking the thresholds for binarizing equally distributed from minus the standard deviation to plus the standard deviation around the mean of the activation. After which the betas are calculated using linear regression again. This cannot lead to good efficiency during inference as the linear regression still has to be done at each layer every iteration. We thought this is an acceptable compromise, since, without the efficient operators, we couldn't show the actual efficiency benefits in practice anyway and this should still give comparable results in terms of accuracy.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Initialization">Initialization<a class="anchor-link" href="#Initialization"> </a></h2><p>A very useful aspect of ABC-Net is the fact that it uses multiple bits to approximate the original real-valued weights. This means that if you have a trained model and you want to replace the normal convolutions with the approximated convolutions of ABC-Net, you can use the weights of the original model as the initialization of the new one. Some training is still needed to finetune the weights to the new architecture, but much less than with random initialization. Once we implemented this, we only trained each network for 1 epoch in our testing. This helped with testing and debugging the network, since when using binarization for each layer in the network the linear regression had to be performed every iteration. This made the training 2 to 12 times slower, depending on the number of bits used for the binarization. This would have meant that without this initialization some networks would have needed to train for 12 days, which wouldn't be feasible for this project.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Normalization">Normalization<a class="anchor-link" href="#Normalization"> </a></h2><p>As mentioned in section "ABC-Net", one of the things that is needed to be able to binarize the activations is normalization. In ABC-Net this is done using batch normalization. However, this was not possible in our case since the GPU we used, did not have enough memory. When only binarizing the weights, batches of 8 could be used, which is already very small for batch normalization to work well. When we also binarized the activations even more memory was needed, since the binarized tensors still needed to be stored as 32-bit floats. This meant that we could only use a batch size of 1. Therefore we looked into 3 other types of normalization, namely, instance normalization, group normalization and layer normalization. Unfortunately, all of the normalization techniques hurt the performance of the model, with really low PSNR's even when not even binarizing the activations yet. We speculate that this might have to do with the fact that we use the weights of the trained LSID model as initialization. Adding normalization in can cause the activations to change significantly, which means that the weights also need to adapt.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Chosen-hyperparameters">Chosen hyperparameters<a class="anchor-link" href="#Chosen-hyperparameters"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Results">Results<a class="anchor-link" href="#Results"> </a></h1><p>The results are shown in the table below and the visual representation can be seen by running the following code cell. When a cell in the table contains a "x", that means the specific part has not been binarized and thus contains the full-precision values.
Just as a reminder: $M$ are the amount of bits used for the weight approximation, $N$ is the amount of bits used for approximating the activations. The theoretical speedup has been calculated per setting as described earlier in this blog.</p>
<table>
<thead><tr>
<th></th>
<th>M</th>
<th>N</th>
<th>PSNR</th>
<th style="text-align:center">Theoretical speedup</th>
</tr>
</thead>
<tbody>
<tr>
<td>Results by [1], in TensorFlow</td>
<td>x</td>
<td>x</td>
<td>28.88</td>
<td style="text-align:center">1x</td>
</tr>
<tr>
<td>Results of [3], in PyTorch</td>
<td>x</td>
<td>x</td>
<td>28.55</td>
<td style="text-align:center">1x</td>
</tr>
<tr>
<td>ABCLSID</td>
<td>3</td>
<td>x</td>
<td>27.6</td>
<td style="text-align:center">0.66x</td>
</tr>
<tr>
<td></td>
</tr>
<tr>
<td>ABCLSID</td>
<td>1</td>
<td>x</td>
<td>25.70</td>
<td style="text-align:center">2x</td>
</tr>
<tr>
<td></td>
</tr>
<tr>
<td>ABCLSID</td>
<td>1</td>
<td>1</td>
<td>...</td>
<td style="text-align:center">53.01x</td>
</tr>
<tr>
<td>ABCLSID</td>
<td>3</td>
<td>1</td>
<td>...</td>
<td style="text-align:center">17.74x</td>
</tr>
<tr>
<td>ABCLSID</td>
<td>3</td>
<td>3</td>
<td>20.93</td>
<td style="text-align:center">5.98x</td>
</tr>
<tr>
<td>ABCLSID</td>
<td>3</td>
<td>5</td>
<td>...</td>
<td style="text-align:center">3.63x</td>
</tr>
<tr>
<td>ABCLSID</td>
<td>5</td>
<td>1</td>
<td>16.61</td>
<td style="text-align:center">10.68x</td>
</tr>
<tr>
<td>ABCLSID</td>
<td>5</td>
<td>3</td>
<td>21.19</td>
<td style="text-align:center">3.63x</td>
</tr>
<tr>
<td>ABCLSID</td>
<td>5</td>
<td>5</td>
<td>21.86</td>
<td style="text-align:center">2.22x</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">widgets</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pylab</span>

<span class="c1"># Image(&#39;example.png&#39;)</span>
<span class="n">tb</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">TabBar</span><span class="p">([</span><span class="s2">&quot;M=1, N=x&quot;</span>                         <span class="p">,</span><span class="s2">&quot;M=1, N=1&quot;</span><span class="p">,</span>                         <span class="s2">&quot;M=3, N=x&quot;</span>                         <span class="p">,</span><span class="s2">&quot;M=3, N=3&quot;</span>                         <span class="p">,</span><span class="s2">&quot;M=5, N=3&quot;</span>                         <span class="p">,</span><span class="s2">&quot;M=5, N=5&quot;</span><span class="p">])</span>
<span class="n">image_ids</span> <span class="o">=</span>         <span class="p">[</span><span class="s2">&quot;1XZuoIFNmI65Wuhnv4lEVDMP5YvVfy2fV&quot;</span><span class="p">,</span><span class="s2">&quot;1Ypb9LUoJt5V44qu9zHXobbNwoa4NOZcn&quot;</span><span class="p">,</span><span class="s2">&quot;1ihb4AvT8Jjj8g5N0zfLrss7-Bms0Vr5g&quot;</span><span class="p">,</span><span class="s2">&quot;1niA76Jwfp6cIdWmnv70omRFBtAaut1Ub&quot;</span><span class="p">,</span><span class="s2">&quot;1CK_rFLYvjPqkw2j4RIng8grU6ovAATWc&quot;</span><span class="p">,</span><span class="s2">&quot;1BN66SlR4UVNOn09txj-9RQKpC-aKtVPX&quot;</span> <span class="p">]</span>

<span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tb</span><span class="p">):</span>
    <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s2">&quot;https://docs.google.com/u/0/uc?id=&quot;</span><span class="o">+</span><span class="n">image_ids</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">width</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">700</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
&lt;link rel=stylesheet type=text/css href='/nbextensions/google.colab/tabbar.css'&gt;&lt;/link&gt;
</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<script src="/nbextensions/google.colab/tabbar_main.min.js"></script>
</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div id="id29"></div>
</div>

</div>

<div class="output_area">




<div id="bcade08d-4db0-4a38-b65d-ef18fcde2671"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#bcade08d-4db0-4a38-b65d-ef18fcde2671');
window["d81b87c4-bbb3-11ea-a0ea-0242ac1c0002"] = colab_lib.createTabBar({"location": "top", "elementId": "id29", "tabNames": ["M=1, N=x", "M=1, N=1", "M=3, N=x", "M=3, N=3", "M=5, N=3", "M=5, N=5"], "initialSelection": 0, "contentBorder": ["0px"], "contentHeight": ["initial"], "borderColor": ["#a7a7a7"]});
//# sourceURL=js_865dca2455
</script>
</div>

</div>

<div class="output_area">




<div id="2f41e665-bb8c-44ae-b588-7fc08850d3ee"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#2f41e665-bb8c-44ae-b588-7fc08850d3ee');
window["d81c0bc2-bbb3-11ea-a0ea-0242ac1c0002"] = window["id29"].setSelectedTabIndex(0);
//# sourceURL=js_19b1d0e27a
</script>
</div>

</div>

<div class="output_area">




<div id="807e0594-6436-4b58-baf3-6db41693a3bb"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#807e0594-6436-4b58-baf3-6db41693a3bb');
window["d81df73e-bbb3-11ea-a0ea-0242ac1c0002"] = google.colab.output.getActiveOutputArea();
//# sourceURL=js_cb2e2e1c79
</script>
</div>

</div>

<div class="output_area">




<div id="88e2ed78-8359-482d-a869-51d752fbaba3"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#88e2ed78-8359-482d-a869-51d752fbaba3');
window["d81e5990-bbb3-11ea-a0ea-0242ac1c0002"] = document.querySelector("#id29_content_0");
//# sourceURL=js_c7e1808657
</script>
</div>

</div>

<div class="output_area">




<div id="aa0ccb54-079d-4953-bc0f-f313d75430d3"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#aa0ccb54-079d-4953-bc0f-f313d75430d3');
window["d81eb606-bbb3-11ea-a0ea-0242ac1c0002"] = google.colab.output.setActiveOutputArea(window["d81e5990-bbb3-11ea-a0ea-0242ac1c0002"]);
//# sourceURL=js_d5bcb66506
</script>
</div>

</div>

<div class="output_area">




<div id="318a2dc4-1a57-4750-a8c2-be6a5819d217"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#318a2dc4-1a57-4750-a8c2-be6a5819d217');
window["d81ef634-bbb3-11ea-a0ea-0242ac1c0002"] = window["id29"].setSelectedTabIndex(0);
//# sourceURL=js_a33040973c
</script>
</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<img src="https://docs.google.com/u/0/uc?id=1XZuoIFNmI65Wuhnv4lEVDMP5YvVfy2fV" width="1000" height="700" />
</div>

</div>

<div class="output_area">




<div id="ba7ace22-01b7-47eb-97b2-8fd476c53636"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#ba7ace22-01b7-47eb-97b2-8fd476c53636');
window["d8202388-bbb3-11ea-a0ea-0242ac1c0002"] = google.colab.output.setActiveOutputArea(window["d81df73e-bbb3-11ea-a0ea-0242ac1c0002"]);
//# sourceURL=js_98ae6056a1
</script>
</div>

</div>

<div class="output_area">




<div id="6d9b5980-b91c-4d1d-8c74-ac16772069cd"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#6d9b5980-b91c-4d1d-8c74-ac16772069cd');
window["d8225b1c-bbb3-11ea-a0ea-0242ac1c0002"] = google.colab.output.getActiveOutputArea();
//# sourceURL=js_9fa43c7f51
</script>
</div>

</div>

<div class="output_area">




<div id="a22c0ecb-bfad-4c78-b127-df58311200c4"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#a22c0ecb-bfad-4c78-b127-df58311200c4');
window["d822c494-bbb3-11ea-a0ea-0242ac1c0002"] = document.querySelector("#id29_content_1");
//# sourceURL=js_1a42bfc9f8
</script>
</div>

</div>

<div class="output_area">




<div id="35eb2b35-a110-4fe2-a785-2854c14652f2"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#35eb2b35-a110-4fe2-a785-2854c14652f2');
window["d8231020-bbb3-11ea-a0ea-0242ac1c0002"] = google.colab.output.setActiveOutputArea(window["d822c494-bbb3-11ea-a0ea-0242ac1c0002"]);
//# sourceURL=js_58f9e0aaf0
</script>
</div>

</div>

<div class="output_area">




<div id="b1327f1e-1328-4865-a154-0c6131d4ca8a"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#b1327f1e-1328-4865-a154-0c6131d4ca8a');
window["d82362d2-bbb3-11ea-a0ea-0242ac1c0002"] = window["id29"].setSelectedTabIndex(1);
//# sourceURL=js_63b9a4eeb3
</script>
</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<img src="https://docs.google.com/u/0/uc?id=1Ypb9LUoJt5V44qu9zHXobbNwoa4NOZcn" width="1000" height="700" />
</div>

</div>

<div class="output_area">




<div id="26a4c603-0d77-4fe5-a7a5-50a512b2f871"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#26a4c603-0d77-4fe5-a7a5-50a512b2f871');
window["d8247b5e-bbb3-11ea-a0ea-0242ac1c0002"] = google.colab.output.setActiveOutputArea(window["d8225b1c-bbb3-11ea-a0ea-0242ac1c0002"]);
//# sourceURL=js_ada2cace2f
</script>
</div>

</div>

<div class="output_area">




<div id="0a8ee83d-ce83-467f-b07c-bd7e67c20223"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#0a8ee83d-ce83-467f-b07c-bd7e67c20223');
window["d825e160-bbb3-11ea-a0ea-0242ac1c0002"] = google.colab.output.getActiveOutputArea();
//# sourceURL=js_36ffc75b0b
</script>
</div>

</div>

<div class="output_area">




<div id="5c99b83f-6e7e-48b3-a652-01de214f257d"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#5c99b83f-6e7e-48b3-a652-01de214f257d');
window["d8263cf0-bbb3-11ea-a0ea-0242ac1c0002"] = document.querySelector("#id29_content_2");
//# sourceURL=js_174c20c413
</script>
</div>

</div>

<div class="output_area">




<div id="5bd57002-a239-4260-840f-9f5bb1e40c9f"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#5bd57002-a239-4260-840f-9f5bb1e40c9f');
window["d8268732-bbb3-11ea-a0ea-0242ac1c0002"] = google.colab.output.setActiveOutputArea(window["d8263cf0-bbb3-11ea-a0ea-0242ac1c0002"]);
//# sourceURL=js_b0643de7e2
</script>
</div>

</div>

<div class="output_area">




<div id="15a4a0b6-2f4d-4388-89cb-3f56508690bf"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#15a4a0b6-2f4d-4388-89cb-3f56508690bf');
window["d826d1d8-bbb3-11ea-a0ea-0242ac1c0002"] = window["id29"].setSelectedTabIndex(2);
//# sourceURL=js_5823bb3c68
</script>
</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<img src="https://docs.google.com/u/0/uc?id=1ihb4AvT8Jjj8g5N0zfLrss7-Bms0Vr5g" width="1000" height="700" />
</div>

</div>

<div class="output_area">




<div id="c65ae1f2-9371-4ff2-948b-21499fc7969f"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#c65ae1f2-9371-4ff2-948b-21499fc7969f');
window["d828024c-bbb3-11ea-a0ea-0242ac1c0002"] = google.colab.output.setActiveOutputArea(window["d825e160-bbb3-11ea-a0ea-0242ac1c0002"]);
//# sourceURL=js_e5b0161862
</script>
</div>

</div>

<div class="output_area">




<div id="ff8f7175-aaa8-444b-8ae8-5e165ae76d81"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#ff8f7175-aaa8-444b-8ae8-5e165ae76d81');
window["d829692a-bbb3-11ea-a0ea-0242ac1c0002"] = google.colab.output.getActiveOutputArea();
//# sourceURL=js_bf107b5572
</script>
</div>

</div>

<div class="output_area">




<div id="42435da6-de7d-49cd-9be7-5eb8842f491a"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#42435da6-de7d-49cd-9be7-5eb8842f491a');
window["d829bf38-bbb3-11ea-a0ea-0242ac1c0002"] = document.querySelector("#id29_content_3");
//# sourceURL=js_c32df71fed
</script>
</div>

</div>

<div class="output_area">




<div id="b0a35656-570d-4888-8fbd-93ea678f7be4"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#b0a35656-570d-4888-8fbd-93ea678f7be4');
window["d82a0f56-bbb3-11ea-a0ea-0242ac1c0002"] = google.colab.output.setActiveOutputArea(window["d829bf38-bbb3-11ea-a0ea-0242ac1c0002"]);
//# sourceURL=js_fc53105123
</script>
</div>

</div>

<div class="output_area">




<div id="d0c77285-1fc6-4940-a389-53dd013ff0a8"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#d0c77285-1fc6-4940-a389-53dd013ff0a8');
window["d82a5d26-bbb3-11ea-a0ea-0242ac1c0002"] = window["id29"].setSelectedTabIndex(3);
//# sourceURL=js_d6396cac15
</script>
</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<img src="https://docs.google.com/u/0/uc?id=1niA76Jwfp6cIdWmnv70omRFBtAaut1Ub" width="1000" height="700" />
</div>

</div>

<div class="output_area">




<div id="8e25dc29-2aa3-41d4-b028-d1b02cb9fe3d"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#8e25dc29-2aa3-41d4-b028-d1b02cb9fe3d');
window["d82b850c-bbb3-11ea-a0ea-0242ac1c0002"] = google.colab.output.setActiveOutputArea(window["d829692a-bbb3-11ea-a0ea-0242ac1c0002"]);
//# sourceURL=js_d904f5d099
</script>
</div>

</div>

<div class="output_area">




<div id="cbbc668a-289c-40e3-bedf-cbf6e767ceb2"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#cbbc668a-289c-40e3-bedf-cbf6e767ceb2');
window["d82ce01e-bbb3-11ea-a0ea-0242ac1c0002"] = google.colab.output.getActiveOutputArea();
//# sourceURL=js_f5d266a68e
</script>
</div>

</div>

<div class="output_area">




<div id="fcb514f7-9c00-4e13-8e64-053311560359"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#fcb514f7-9c00-4e13-8e64-053311560359');
window["d82d4298-bbb3-11ea-a0ea-0242ac1c0002"] = document.querySelector("#id29_content_4");
//# sourceURL=js_4faa7c3d18
</script>
</div>

</div>

<div class="output_area">




<div id="0d2fa304-183b-45ab-a6ed-fad952baf88a"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#0d2fa304-183b-45ab-a6ed-fad952baf88a');
window["d82d8b68-bbb3-11ea-a0ea-0242ac1c0002"] = google.colab.output.setActiveOutputArea(window["d82d4298-bbb3-11ea-a0ea-0242ac1c0002"]);
//# sourceURL=js_8b81aa8a26
</script>
</div>

</div>

<div class="output_area">




<div id="85ebd0cf-ca93-4af8-b969-a798da051289"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#85ebd0cf-ca93-4af8-b969-a798da051289');
window["d82de1c6-bbb3-11ea-a0ea-0242ac1c0002"] = window["id29"].setSelectedTabIndex(4);
//# sourceURL=js_09c0f58371
</script>
</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<img src="https://docs.google.com/u/0/uc?id=1CK_rFLYvjPqkw2j4RIng8grU6ovAATWc" width="1000" height="700" />
</div>

</div>

<div class="output_area">




<div id="ac887cc7-ef23-43b9-915b-7302aa214ce8"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#ac887cc7-ef23-43b9-915b-7302aa214ce8');
window["d82ef962-bbb3-11ea-a0ea-0242ac1c0002"] = google.colab.output.setActiveOutputArea(window["d82ce01e-bbb3-11ea-a0ea-0242ac1c0002"]);
//# sourceURL=js_14657752f7
</script>
</div>

</div>

<div class="output_area">




<div id="779a05b6-c4fe-427a-9898-f4a0765601f9"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#779a05b6-c4fe-427a-9898-f4a0765601f9');
window["d8305c44-bbb3-11ea-a0ea-0242ac1c0002"] = google.colab.output.getActiveOutputArea();
//# sourceURL=js_c0966ec55a
</script>
</div>

</div>

<div class="output_area">




<div id="591a8198-167c-4a56-889c-a5b86c3300bd"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#591a8198-167c-4a56-889c-a5b86c3300bd');
window["d830b194-bbb3-11ea-a0ea-0242ac1c0002"] = document.querySelector("#id29_content_5");
//# sourceURL=js_aaef009b34
</script>
</div>

</div>

<div class="output_area">




<div id="586d3e69-45f5-4e4c-a4d0-9e5d6740bb50"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#586d3e69-45f5-4e4c-a4d0-9e5d6740bb50');
window["d830f870-bbb3-11ea-a0ea-0242ac1c0002"] = google.colab.output.setActiveOutputArea(window["d830b194-bbb3-11ea-a0ea-0242ac1c0002"]);
//# sourceURL=js_fc8e9a7c87
</script>
</div>

</div>

<div class="output_area">




<div id="ca8c813a-e912-4879-976b-52025f8ddf3a"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#ca8c813a-e912-4879-976b-52025f8ddf3a');
window["d8315612-bbb3-11ea-a0ea-0242ac1c0002"] = window["id29"].setSelectedTabIndex(5);
//# sourceURL=js_262a088d03
</script>
</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<img src="https://docs.google.com/u/0/uc?id=1BN66SlR4UVNOn09txj-9RQKpC-aKtVPX" width="1000" height="700" />
</div>

</div>

<div class="output_area">




<div id="92662054-6928-49bc-b7ff-b68c8495b4dd"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#92662054-6928-49bc-b7ff-b68c8495b4dd');
window["d8326142-bbb3-11ea-a0ea-0242ac1c0002"] = google.colab.output.setActiveOutputArea(window["d8305c44-bbb3-11ea-a0ea-0242ac1c0002"]);
//# sourceURL=js_27bd55cf7c
</script>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Discussion">Discussion<a class="anchor-link" href="#Discussion"> </a></h1><p>From the results section, one thing that is immediately clear, is when we start to binarize the activations, the performance of the network starts to decline. As noted before, we were unable to implement the binarization of the activations in the way we would have wanted. We expect that, given access to a computer with enough resources, the binarization of the activations can be improved further. However, it might also be possible that, with the binarization of these activations, there is a significant amount of information loss. In such case, it can be inferred, that the speedup of the network would be relatively small.</p>
<p>We are quite certain that our implementation  still has some room for improvement. We noticed that for $M,N = 3$, the tensors of the estimated weights and activations only contained 4 or 5 unique values. Having a 3 bit accuracy, it should be possible to achieve 8 unique levels instead. One explanation for this could be that the shifting of parameters to form $\boldsymbol{B}$ is not done optimally. Right now the shifts, which set the thresholds for binarization, are uniformly distributed over a range from minus the standard deviation to plus the standard deviation, as described in ABC-Net. However, since the distribution of weights often resembles a Gaussian distribution, it might be better to put the thresholds around the mean closer together and the thresholds further from the mean further apart, to better match the Gaussian distribution.
Currently, in the interest of time for this project, we were unable to research further into this problem, but we expect that if more of the possible weights occur, that this will lead to a smaller approximation error, which in term might help with performance.</p>
<p>Currently, the network is initialized with the weights from the original TensorFlow model. However, as the original network did not incorporate any normalization techniques, it might be beneficial to train our full-precision PyTorch model with normalization, and use those weights as an initialization in the binarized network.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="References">References<a class="anchor-link" href="#References"> </a></h1><p>[1] C. Chen, Q. Chen, J. Xu, and V. Koltun, “Learning to see in the dark”, CoRR, vol. abs/1805.01934,2018. [Online]. Available: <a href="http://arxiv.org/abs/1805.019341">http://arxiv.org/abs/1805.019341</a></p>
<p>[2] X. Lin, C. Zhao, and W. Pan, “Towards accurate binary convolutional neural network”, CoRR, vol.abs/1711.11294, 2017. [Online]. Available: <a href="http://arxiv.org/abs/1711.112941">http://arxiv.org/abs/1711.112941</a></p>
<p>[3] Cydonia, "Learning to See in the Dark in PyTorch", 2018.  [Online].  Available:   <a href="https://github.com/cydonia999/Learning_to_See_in_the_Dark_PyTorch">https://github.com/cydonia999/Learning_to_See_in_the_Dark_PyTorch</a></p>
<p>[4] Mylio,2020.[Online].Available:<a href="https://focus.mylio.com/tech-today/how-many-photos-will-be-taken-in-2020">https://focus.mylio.com/tech-today/how-many-photos-will-be-taken-in-2020</a></p>
<p>[5]  M.   Rastegari,   V.   Ordonez,   J.   Redmon,   and   A.   Farhadi,   “Xnor-net:Imagenet   classification using  binary  convolutional  neural  networks”, CoRR,  vol.  abs/1603.05279,  2016.  [Online].  Available:<a href="http://arxiv.org/abs/1603.052791">http://arxiv.org/abs/1603.052791</a></p>
<p>[6]  E. Nurvitadhi, D. Sheffield, Jaewoong Sim, A. Mishra, G. Venkatesh, and D. Marr, “Accelerating bina-rized neural networks:  Comparison of fpga,  cpu,  gpu,  and asic,”  in2016 International Conference onField-Programmable Technology (FPT), 2016, pp. 77–84.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Want to see more reproductions of this and other papers? Check <a href="https://reproduced-papers.firebaseapp.com/">https://reproduced-papers.firebaseapp.com/</a></p>
<hr />

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="sfalkena/blogs"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blogs/fastpages/jupyter/2020/07/01/LearningToEfficientlySeeInTheDark.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blogs/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blogs/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blogs/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Where I will keep my blogs of projects that I did.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/blogs/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/blogs/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
