{
  
    
        "post0": {
            "title": "Title",
            "content": ". Running the experiment on Google Colab . from torchvision import transforms from google.colab import drive from torch.utils import data from PIL import Image import matplotlib.pyplot as plt import PIL.Image as pil_image import torch.nn as nn import torchvision import numpy as np import torch import time import cv2 import os . drive.mount(&#39;/content/gdrive&#39;) path =&#39;/content/gdrive/My Drive/deep_learning_group_7/Final&#39; os.chdir(path) . Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(&#34;/content/gdrive&#34;, force_remount=True). . from IPython.display import HTML HTML(filename = path + &#39;/interactive_image.html&#39;) . &lt;!DOCTYPE html&gt; . Comparison of low resolution and upscaled image . Click and slide the blue slider to compare two images: . . ESPCN Architecture . r = 3 #scaling factor class SuperResConvNet(nn.Module): def __init__(self): &#39;Initialize layers&#39; super(SuperResConvNet, self).__init__() self.conv1 = nn.Conv2d(1,64, kernel_size=5, stride=1, padding=2) self.conv2 = nn.Conv2d(64,32, kernel_size=3, stride=1, padding=1) self.conv3 = nn.Conv2d(32,1*r**2, kernel_size=3, stride=1, padding=1) self.upsample = nn.PixelShuffle(r) def forward(self, y): &#39;Define forward pass&#39; y = torch.tanh(self.conv1(y)) y = torch.tanh(self.conv2(y)) y = self.conv3(y) y = self.upsample(y) return y # Check for GPU availibility if torch.cuda.is_available(): print(&quot;Using GPU&quot;) device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) # Define model and send to device ConvNet = SuperResConvNet() ConvNet.to(device) . Using GPU . SuperResConvNet( (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (conv2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv3): Conv2d(32, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (upsample): PixelShuffle(upscale_factor=3) ) . Functions . # Define directories slide_subfolders = [&#39;yang91/T91/&#39;, #trainingset &#39;x3.0/Set5/&#39;, &#39;x3.0/Set14/&#39;, # Testsets &#39;x3.0/BSD300/&#39;, &#39;x3.0/BSD500/&#39;, &#39;x3.0/SuperText136/&#39;] #width and height of patches x = 17 class Slide: &#39;Combines functions for loading and preparing images for training and testing&#39; def __init__(self, path, slide_subfolders, count): self.count = count self.dir = path + &#39;/CVPR2016_ESPCN_OurBenchMarkResult/Ours/&#39; + slide_subfolders self.namelist = self.deleteLRImage() if count == 0: self.patchlist = self.patchlist_get() def removePng(f): &#39;returns filename without png&#39; filename_parts = f[:-4] return filename_parts def getList(self): &#39;returns list of filenames&#39; return [Slide.removePng(f) for f in os.listdir(self.dir) if f.endswith(&quot;.png&quot;)] def deleteLRImage(self): &#39;deletes LR images from list if they are already made (which they are)&#39; name_list_2 = Slide.getList(self) name_list_1 = [x for x in name_list_2 if &quot;lr&quot; not in x ] name_list = [x for x in name_list_1 if &quot;lowRes&quot; not in x ] return name_list def getPatchList(self, img_name): &#39;returns patch list of one image&#39; #get filenames and load images filename = self.dir + img_name lr_img = cv2.cvtColor(cv2.imread(filename+&#39;_lr.png&#39;), cv2.COLOR_BGR2RGB) #parameters stride_lr = x-np.sum((5%2,3%2,3%2)) tot_img_d = int(lr_img.shape[0]/stride_lr), int(lr_img.shape[1]/stride_lr) #amount of image in height and width respectively tot_img = tot_img_d[0]*tot_img_d[1] #total amount of images #create list for current image patch_list = [] for i in range(tot_img_d[0]-1): for j in range(tot_img_d[1]-1): patch_list.append([img_name, i,j]) return patch_list def patchlist_get(self): &#39;create patch_list&#39; patch_list = [] for i in range(len(self.namelist)): patch_list.extend(self.getPatchList(self.namelist[i])) print(&#39;Found&#39;, len(patch_list), &#39;trainable patches out of&#39;, len(self.namelist), &#39;images.&#39;) return patch_list def createPatch(self, name): &#39;returns a patch&#39; img_name = name[0] patch_name = name[1], name[2] #get corresponding images filename = self.dir + img_name hr_img = cv2.cvtColor(cv2.imread(filename+&#39;.png&#39;), cv2.COLOR_BGR2YCrCb)[:,:,0] # only get Y channel from YCrCb lr_img = cv2.cvtColor(cv2.imread(filename+&#39;_lr.png&#39;), cv2.COLOR_BGR2YCR_CB)[:,:,0] #create hr patch stride_hr = (x-np.sum((5%2,3%2,3%2)))*r hr_patch = hr_img[stride_hr*patch_name[0]:(stride_hr*patch_name[0]+17*r),stride_hr*patch_name[1]:(stride_hr*patch_name[1]+17*r)] #create lr patch stride_lr = x-np.sum((5%2,3%2,3%2)) lr_patch = lr_img[stride_lr*patch_name[0]:(stride_lr*patch_name[0]+17),stride_lr*patch_name[1]:(stride_lr*patch_name[1]+17)] return lr_patch, hr_patch def bicubic_upsampling(self, img_n): &#39;Loading high and low resolution image and do a bicubic upsampling of the low resolution image&#39; #get corresponding images filename = self.dir + img_n hr_img = Image.open(filename + &#39;.png&#39;).convert(&#39;YCbCr&#39;) lr_img = Image.open(filename[:-9] + &#39;-lowRes.png&#39;).convert(&#39;YCbCr&#39;) #upsample bicubic = lr_img.resize(hr_img.size, Image.BICUBIC) return lr_img, bicubic, hr_img # return as pil images def getSlideList(slide_subfolders, path): &#39;Load Slide class for different directories&#39; slides = [] for i, slide in enumerate(slide_subfolders): slides.append(Slide(path, slide, i)) return slides slides = getSlideList(slide_subfolders, path) . Found 1734 trainable patches out of 91 images. . PSNR calculation . def psnr_from_mselist(mse_list): &#39;Calculate PSNR&#39; mse = np.mean(mse_list) if mse == 0: return float(&#39;inf&#39;) else: return 20*np.log10(255/np.sqrt(mse)) def calc_mse(img_pred, img_hr): &#39;Calculate MSE&#39; return np.mean((img_pred*255 - img_hr*255)**2) . Obtaining low resolution images . ## ONLY RUN THIS CELL IF LOW RESOLUTION IMAGES ARE NOT PRESENT IN THE DIRECTORY def createLowRes(img_name, dir_91): &#39;saves low resolution image&#39; # Call HR image filename = dir_91 + img_name + &#39;.png&#39; hr_img = cv2.cvtColor(cv2.imread(filename), cv2.COLOR_BGR2RGB) # Blur HR image blur_img = cv2.GaussianBlur(hr_img,(5,5),0) # Apply subsampling lr_img = blur_img[::r,::r] # Save lr_img im = Image.fromarray(lr_img) im.save(dir_91 + img_name + &#39;_lr.png&#39;) for i in range(len(slides[0].namelist)): dir_91 = path + &#39;/CVPR2016_ESPCN_OurBenchMarkResult/Ours/&#39; + slide_subfolders[0] createLowRes(slides[0].namelist[i], dir_91) . . Training . Hyperparameter settings . # Loss and optimizer criterion = nn.MSELoss() optimizer = torch.optim.Adam(ConvNet.parameters(), lr=0.01) # Parameters num_epoch = 5000 #Amount of epochs batch_size = 16 #Batch size train_val_ratio = 0.95 #Training validation ratio # Scheduler for dynamic reduction of the learning rate threshold_mu = 1e-6 # Treshold for decreasing learning rate. factor_value = 0.8 # Amount of decay per step, new lr = factor_value*lr. scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=&#39;min&#39;, factor=factor_value, patience=2, threshold=threshold_mu, min_lr=0.0001, eps=1e-08, verbose=True) . . Data loader . class DataGenerator(data.Dataset): &#39;Generates the dataset that is used for training the ESPCN&#39; def __init__(self, slides): self.slides = slides self.transform = torchvision.transforms.Compose([ torchvision.transforms.ToTensor()]) def __len__(self): &#39;Returns the amount of patches&#39; return len(self.slides.patchlist) def __getitem__(self, idx): &#39;Returns low and high resolution patches in the form of tensors&#39; lr_patch, hr_patch = self.slides.createPatch(self.slides.patchlist[idx]) # transform images to pytorch tensors lr_patch = self.transform(lr_patch) hr_patch = self.transform(hr_patch) return lr_patch, hr_patch # Put DataGenerator in DataLoader full_dataset = DataGenerator(slides[0]) # Split between training and validation set train_size = int(train_val_ratio * len(full_dataset)) validation_size = len(full_dataset) - train_size training_set, validation_set = torch.utils.data.random_split(full_dataset, [train_size, validation_size]) # Create training and validation data loaders training_generator = data.DataLoader(training_set, batch_size=batch_size, num_workers=batch_size, shuffle=&#39;True&#39;) validation_generator = data.DataLoader(validation_set, batch_size=1, num_workers=1, shuffle=&#39;False&#39;) . Training the model . # Initializing lists used for saving losses and PSNR loss_list = [] epoch_loss_list = [] val_loss_list = [] validation_psnr = [] # Start timer t0 = time.time() # Training loop for epoch in range(num_epoch): # Switch to training mode ConvNet.train() for i, (lr_patch, hr_patch) in enumerate(training_generator): # Transfer training data to active device lr_patch, hr_patch = lr_patch.to(device), hr_patch.to(device) # Run the forward pass outputs = ConvNet(lr_patch) loss = criterion(outputs, hr_patch) loss_list.append(loss.item()) # Backprop and perform Adam optimisation optimizer.zero_grad() loss.backward() optimizer.step() # Save loss every epoch epoch_loss = np.sum(loss_list)/len(training_generator) epoch_loss_list.append(epoch_loss) # Print epoch loss every 50 epochs if epoch % 50 == 0: print(&quot;Epoch&quot;, epoch, &quot;loss: {}&quot;.format(epoch_loss)) # Save model every 1000 epochs (in case Google Colab stops runtime) # if epoch % 1000 == 999: # model_name = &#39;final_&#39; + str(epoch+1) + &#39;_epochs&#39; # path_model = path + &#39;/saved_models/&#39; + model_name # torch.save(ConvNet.state_dict(), path_model) # print(&#39;Model saved as: &#39;, model_name) # Step to next step of lr-scheduler scheduler.step(epoch_loss) loss_list = [] # Enter validation mode ConvNet.eval() # Keep track of mse for every patch, to collectively calculate PSNR per epoch epoch_mse = [] with torch.no_grad(): for i, (lr_patch, hr_patch) in enumerate(validation_generator): # Transfer training data to active device (GPU) lr_patch, hr_patch = lr_patch.to(device), hr_patch.to(device) # Predict output img_pred = ConvNet(lr_patch) # Calculate validation loss loss = criterion(img_pred, hr_patch) loss_list.append(loss.item()) # Calculate mse for every sample img_pred = img_pred[0].cpu().numpy() hr_patch = hr_patch[0].cpu().numpy() epoch_mse.append(calc_mse(img_pred, hr_patch)) # Calculate validation psnr on the complete epoch from all individual MSE&#39;s val_psnr = psnr_from_mselist(np.array(epoch_mse)) validation_psnr.append(val_psnr) # Save validation loss every epoch val_loss = np.sum(loss_list)/len(validation_generator) val_loss_list.append(val_loss) loss_list = [] print(&#39;Training took {} seconds&#39;.format(time.time() - t0)) print(&#39;Seconds per epoch:&#39;,(time.time()-t0)/num_epoch) . Plot training results . # Show training and validation loss of current model in memory plt.figure(figsize=(8,8)) plt.subplot(2,1,1) plt.title(&#39;Loss per epoch&#39;) plt.plot(np.arange(num_epoch), epoch_loss_list, label=&#39;Training loss&#39;) plt.plot(np.arange(num_epoch), val_loss_list, label=&#39;Validation loss&#39;) plt.yscale(&quot;log&quot;) plt.legend() plt.subplot(2,1,2) plt.title(&#39;PSNR for validation data&#39;) plt.plot(np.arange(num_epoch), validation_psnr) plt.show() . Testing . Loading the model . # Load weight in earlier defined model model_name = &#39;final_test_5000_epochsweights&#39; path_model = path + &#39;/saved_models/&#39; + model_name ConvNet.load_state_dict(torch.load(path_model)) . &lt;All keys matched successfully&gt; . Test image plot function . def generate_figure(lr_img, sr_img, hr_img): &#39;Show the low resolution, upscaled and high resolution image&#39; f = plt.figure(figsize=(8*3,8)) f.add_subplot(1, 3, 1) plt.imshow(transforms.ToPILImage(&#39;YCbCr&#39;)(lr_img[0]).convert(&#39;RGB&#39;)) f.add_subplot(1, 3, 2) plt.imshow(transforms.ToPILImage(&#39;YCbCr&#39;)(sr_img[0]).convert(&#39;RGB&#39;)) f.add_subplot(1, 3, 3) plt.imshow(transforms.ToPILImage(&#39;YCbCr&#39;)(hr_img[0]).convert(&#39;RGB&#39;)) . Data loader . class DataGenerator_test(data.Dataset): &#39;Generates the dataset used for testing&#39; def __init__(self, slides): self.slides = slides self.transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()]) def __len__(self): &#39;Returns the amount of test images&#39; return len(self.slides.namelist) def __getitem__(self, idx): &#39;Returns low, upsampled and high resolution testing images in the form of tensors&#39; lr_img, bicubic, hr_img = self.slides.bicubic_upsampling(self.slides.namelist[idx]) lr_img = self.transform(lr_img) hr_img = self.transform(hr_img) bicubic = self.transform(bicubic) return lr_img, bicubic, hr_img ## Put DataGenerator in DataLoader def DataGenerator(slides): &#39;Create dataloader for every test directory&#39; test_set = DataGenerator_test(slides) test_generator = data.DataLoader(test_set, batch_size=1, shuffle=False) return test_generator . Testing the model . import random as rnd rnd.seed(9) # Loop over test data sets for j in range(len(slides[1:])): # Load data test_generator = DataGenerator(slides[j+1]) show = rnd.randint(0,len(test_generator)) # Define lists outputs = [] mse_list = [] time_list = [] # Main testing loop for i, (lr_img, bicubic, hr_img) in enumerate(test_generator): with torch.no_grad(): # Only test on the Y (intensity channel) to_network = (lr_img[:,0,:,:]).unsqueeze(0) # Start timer start_time = time.time() # Run the forward pass outputs = ConvNet.cpu()(to_network) img_pred = outputs[0] # Substitute the Y channel from bicubic with the one outputted by the model bicubic[:,0,:,:] = img_pred #Stop timer elapsed_time = time.time() - start_time time_list.append(elapsed_time) # Calculate MSE mse = calc_mse(img_pred.numpy(), hr_img.numpy()[:,0,:,:]) mse_list.append(mse.item()) #Show one random image from every dataset if i == show: generate_figure(lr_img, bicubic, hr_img) # Printing the results print(slide_subfolders[j+1], &#39;: PSNR&#39;, psnr_from_mselist(mse_list), &#39;Average time&#39; ,np.mean(time_list)) . Output hidden; open in https://colab.research.google.com to view. .",
            "url": "https://sfalkena.github.io/ESPCN_reproduction/2020/04/19/ESPCN_reproduction.html",
            "relUrl": "/2020/04/19/ESPCN_reproduction.html",
            "date": " • Apr 19, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://sfalkena.github.io/ESPCN_reproduction/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://sfalkena.github.io/ESPCN_reproduction/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi! Glad that you want to know more about me! My name is Sieger Falkena, I am 24 years old and currently studying my Masters Embedded Systems. I made this website to show some of the work that I did during my studies. More about me will follow soon :) . Just so you know, this website is powered by fastpages 1. Take a look, its cool :) . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://sfalkena.github.io/ESPCN_reproduction/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://sfalkena.github.io/ESPCN_reproduction/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}