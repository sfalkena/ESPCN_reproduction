<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Reproducing “Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network” | Sieger Falkena</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Reproducing “Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network”" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This article describes a replication of Table 1 from the Real-time Single Image Super Resolution paper trained on the yang91 dataset with a validation on different datasets." />
<meta property="og:description" content="This article describes a replication of Table 1 from the Real-time Single Image Super Resolution paper trained on the yang91 dataset with a validation on different datasets." />
<link rel="canonical" href="https://sfalkena.github.io/ESPCN_reproduction/fastpages/jupyter/2020/04/20/ESPCN_reproduction.html" />
<meta property="og:url" content="https://sfalkena.github.io/ESPCN_reproduction/fastpages/jupyter/2020/04/20/ESPCN_reproduction.html" />
<meta property="og:site_name" content="Sieger Falkena" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-20T00:00:00-05:00" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://sfalkena.github.io/ESPCN_reproduction/fastpages/jupyter/2020/04/20/ESPCN_reproduction.html"},"description":"This article describes a replication of Table 1 from the Real-time Single Image Super Resolution paper trained on the yang91 dataset with a validation on different datasets.","@type":"BlogPosting","url":"https://sfalkena.github.io/ESPCN_reproduction/fastpages/jupyter/2020/04/20/ESPCN_reproduction.html","headline":"Reproducing “Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network”","dateModified":"2020-04-20T00:00:00-05:00","datePublished":"2020-04-20T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/ESPCN_reproduction/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://sfalkena.github.io/ESPCN_reproduction/feed.xml" title="Sieger Falkena" /><link rel="shortcut icon" type="image/x-icon" href="/ESPCN_reproduction/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Reproducing “Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network” | Sieger Falkena</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Reproducing “Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network”" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This article describes a replication of Table 1 from the Real-time Single Image Super Resolution paper trained on the yang91 dataset with a validation on different datasets." />
<meta property="og:description" content="This article describes a replication of Table 1 from the Real-time Single Image Super Resolution paper trained on the yang91 dataset with a validation on different datasets." />
<link rel="canonical" href="https://sfalkena.github.io/ESPCN_reproduction/fastpages/jupyter/2020/04/20/ESPCN_reproduction.html" />
<meta property="og:url" content="https://sfalkena.github.io/ESPCN_reproduction/fastpages/jupyter/2020/04/20/ESPCN_reproduction.html" />
<meta property="og:site_name" content="Sieger Falkena" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-20T00:00:00-05:00" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://sfalkena.github.io/ESPCN_reproduction/fastpages/jupyter/2020/04/20/ESPCN_reproduction.html"},"description":"This article describes a replication of Table 1 from the Real-time Single Image Super Resolution paper trained on the yang91 dataset with a validation on different datasets.","@type":"BlogPosting","url":"https://sfalkena.github.io/ESPCN_reproduction/fastpages/jupyter/2020/04/20/ESPCN_reproduction.html","headline":"Reproducing “Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network”","dateModified":"2020-04-20T00:00:00-05:00","datePublished":"2020-04-20T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://sfalkena.github.io/ESPCN_reproduction/feed.xml" title="Sieger Falkena" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/ESPCN_reproduction/">Sieger Falkena</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/ESPCN_reproduction/about/">About Me</a><a class="page-link" href="/ESPCN_reproduction/search/">Search</a><a class="page-link" href="/ESPCN_reproduction/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Reproducing &quot;Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network&quot;</h1><p class="page-description">This article describes a replication of Table 1 from the Real-time Single Image Super Resolution paper trained on the yang91 dataset with a validation on different datasets.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-04-20T00:00:00-05:00" itemprop="datePublished">
        Apr 20, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      29 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/ESPCN_reproduction/categories/#fastpages">fastpages</a>
        &nbsp;
      
        <a class="category-tags-link" href="/ESPCN_reproduction/categories/#jupyter">jupyter</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/sfalkena/ESPCN_reproduction/tree/master/_notebooks/2020-04-20-ESPCN_reproduction.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/ESPCN_reproduction/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/sfalkena/ESPCN_reproduction/master?filepath=_notebooks%2F2020-04-20-ESPCN_reproduction.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/ESPCN_reproduction/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/sfalkena/ESPCN_reproduction/blob/master/_notebooks/2020-04-20-ESPCN_reproduction.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/ESPCN_reproduction/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-04-20-ESPCN_reproduction.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="By-Luuk-Balkenende,-Sieger-Falkena-and-Luc-Kloosterman">By Luuk Balkenende, Sieger Falkena and Luc Kloosterman<a class="anchor-link" href="#By-Luuk-Balkenende,-Sieger-Falkena-and-Luc-Kloosterman"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The text above is for generation of fastpages, and not relevant for the implementation</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This article describes a replication of Table 1 from the <a href="https://arxiv.org/pdf/1609.05158v2.pdf">Real-time Single Image Super-Resolution</a> [1] paper trained on the yang91 dataset with a validation on different datasets. The table below shows the original table from the paper and highlights the results which are attempted to reproduce.</p>
<p><img src="https://drive.google.com/uc?id=15Xbeue7JMrNqitq5OmSV7wPd8CF89miu" alt="table 1" /></p>
<p>The reproduction is done using a Pytorch implementation which is written from scratch. Only the information given in the paper is used. Some hyperparameters are tuned as those are not mentioned in the paper. The code is developed on Google Colab and is compatible to run with a GPU.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Problem-definition-of-the-paper">Problem definition of the paper<a class="anchor-link" href="#Problem-definition-of-the-paper"> </a></h1><p>Several models which are capable of upscaling single images already exist. [2] [3] [4] However, in these methods the super-resolution (SR) operation is performed in high resolution (HR) space. According to the writers of the paper: <em>'this is sub-optimal and adds computation complexity'</em>. The goal is to find a more efficient method to upscale images. More specifically, the goal is to make this process fast enough so that it can be applied real-time to video material.</p>
<h2 id="Experiment-setup-as-proposed-by-the-authors">Experiment setup as proposed by the authors<a class="anchor-link" href="#Experiment-setup-as-proposed-by-the-authors"> </a></h2><p>The authors of the paper propose a CNN architecture where the feature maps are extracted in low resolution (LR) space. Only at the very end of the network the resolution will be increased. The advantage of this network is two fold: 
The computational complexity of the whole model is low.
Better and more complex SR operations are learned compared to other models.</p>
<p>The proposed CNN architecture, called ESPCN, is shown below. As can be seen, the ESPCN consists of three convolutional layers, two normal convolutional layers and one sub-pixel convolutional layer which aggregates the feature maps from LR space and performs the SR operation. This last layer is called the sub-pixel convolution layer. In the first layer the Y channel of a YCrCb LR image is taken as the input and is convolved with a kernel size of 5x5 to 64 different output channels. After this, two convolutional layers are used with a kernel size of 3x3 which reduces the number of channels to the square of the scaling factor. The last layer shuffles the pixels to obtain the final SR image.</p>
<p><img src="https://drive.google.com/uc?id=1_XmEeASLK9a94dRA9y7wNdBqIMJC5SDQ" alt="architecture" /></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Implementation-of-the-code">Implementation of the code<a class="anchor-link" href="#Implementation-of-the-code"> </a></h1><p>The actual code is implemented from scratch in this notebook, where only the information given in the paper is used. In this section, the steps which are taken in order to reproduce the desired results of the paper together with the python code are explained.
Running the experiment on Google Colab
The notebook is running remotely on the Google Colab platform. Therefore, to save and access the trained model, we needed to mount the Google drive. We used the following code snippet to set up a local drive on our computer. Furthermore, the packages needed to run the whole notebook are imported.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />
<h2 id="Running-the-experiment-on-Google-Colab">Running the experiment on Google Colab<a class="anchor-link" href="#Running-the-experiment-on-Google-Colab"> </a></h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">drive</span>
<span class="kn">from</span> <span class="nn">torch.utils</span> <span class="kn">import</span> <span class="n">data</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">PIL.Image</span> <span class="k">as</span> <span class="nn">pil_image</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">os</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>If you want to run the notebook, please download the neccesary files from <a href="https://github.com/sfalkena/ESPCN_reproduction/tree/master/_notebooks">Github</a> and make a copy from this notebook!</strong></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">drive</span><span class="o">.</span><span class="n">mount</span><span class="p">(</span><span class="s1">&#39;/content/gdrive&#39;</span><span class="p">)</span>
<span class="n">path</span> <span class="o">=</span><span class="s1">&#39;/content/gdrive/My Drive/deep_learning_group_7/Final&#39;</span>
<span class="n">os</span><span class="o">.</span><span class="n">chdir</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>
<span class="n">HTML</span><span class="p">(</span><span class="n">filename</span> <span class="o">=</span> <span class="n">path</span> <span class="o">+</span> <span class="s1">&#39;/interactive_image.html&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="ESPCN-Architecture">ESPCN Architecture<a class="anchor-link" href="#ESPCN-Architecture"> </a></h2><p>Below, the ESPCN architecture as defined in the paper can be found. For the activation function a tanh is used, as the authors indicated this will lead to better results. The final sub-pixel convolution layer is divided into a normal convolutional layer (conv3) and a layer which performs the SR operation (upsample). Next to defining the model this cell checks the availability of a GPU and stores the model on the GPU if it is available. Note that for the implementation, an upscaling factor of 3 is used.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">r</span> <span class="o">=</span> <span class="mi">3</span> <span class="c1">#scaling factor</span>

<span class="k">class</span> <span class="nc">SuperResConvNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="s1">&#39;Initialize layers&#39;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">SuperResConvNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="mi">1</span><span class="o">*</span><span class="n">r</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">upsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PixelShuffle</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="s1">&#39;Define forward pass&#39;</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span>

<span class="c1"># Check for GPU availability</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Using GPU&quot;</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

<span class="c1"># Define model and send to device</span>
<span class="n">ConvNet</span> <span class="o">=</span> <span class="n">SuperResConvNet</span><span class="p">()</span>
<span class="n">ConvNet</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Datasets-and-data-processing">Datasets and data processing<a class="anchor-link" href="#Datasets-and-data-processing"> </a></h2><p>In order to train the above defined model, the images in the yang91 dataset are downsampled by a factor of 3 and saved on the drive. More about this operation can be found in the next section.</p>
<p>The model will not train on the whole images, but on small parts of the images, as explained in the paper. We refer to these small parts as patches in this reproduction. In order to reduce memory issues, it has been chosen to only save a list of names of the available patches per image, instead of the individual patches themselves. As can be seen in the code snippet below, the function <code>getPatchList</code> is searching for the available patches per image, while <code>PatchList_get</code> is saving the information of the former function into a list. During training time, the function <code>createPatch</code> is used to transform the entries of the list of available patches into real image patches. Finally, the bicubic upsampling function is used to load low resolution images and upsample them to high resolution images. Later, during visualisation, the luminance channel is substituted with the one upsampled by the model, while the Cr and Cb channels are kept. This makes that the images only trained on luminance can be displayed in color. Noticeably, multiple functions need to be called on the sets of images from different directories. For simplicity, all of these functions are accommodated in one class.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Define directories</span>
<span class="n">slide_subfolders</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;yang91/T91/&#39;</span><span class="p">,</span> <span class="c1">#trainingset</span>
                    <span class="s1">&#39;x3.0/Set5/&#39;</span><span class="p">,</span> <span class="s1">&#39;x3.0/Set14/&#39;</span><span class="p">,</span> <span class="c1"># Testsets</span>
                    <span class="s1">&#39;x3.0/BSD300/&#39;</span><span class="p">,</span> <span class="s1">&#39;x3.0/BSD500/&#39;</span><span class="p">,</span> <span class="s1">&#39;x3.0/SuperText136/&#39;</span><span class="p">]</span> 

<span class="c1">#width and height of patches</span>
<span class="n">x</span> <span class="o">=</span> <span class="mi">17</span>      

<span class="k">class</span> <span class="nc">Slide</span><span class="p">:</span>
  <span class="s1">&#39;Combines functions for loading and preparing images for training and testing&#39;</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">slide_subfolders</span><span class="p">,</span> <span class="n">count</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">=</span> <span class="n">count</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dir</span> <span class="o">=</span> <span class="n">path</span> <span class="o">+</span> <span class="s1">&#39;/CVPR2016_ESPCN_OurBenchMarkResult/Ours/&#39;</span> <span class="o">+</span> <span class="n">slide_subfolders</span>
    
    <span class="bp">self</span><span class="o">.</span><span class="n">namelist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">deleteLRImage</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">count</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">patchlist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">patchlist_get</span><span class="p">()</span>
      
  <span class="k">def</span> <span class="nf">removePng</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
    <span class="s1">&#39;returns filename without png&#39;</span>
    <span class="n">filename_parts</span> <span class="o">=</span> <span class="n">f</span><span class="p">[:</span><span class="o">-</span><span class="mi">4</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">filename_parts</span>

  <span class="k">def</span> <span class="nf">getList</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="s1">&#39;returns list of filenames&#39;</span> 
    <span class="k">return</span> <span class="p">[</span><span class="n">Slide</span><span class="o">.</span><span class="n">removePng</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dir</span><span class="p">)</span> <span class="k">if</span> <span class="n">f</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.png&quot;</span><span class="p">)]</span>

  <span class="k">def</span> <span class="nf">deleteLRImage</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="s1">&#39;deletes LR images from list if they are already made (which they are)&#39;</span>
    <span class="n">name_list_2</span> <span class="o">=</span> <span class="n">Slide</span><span class="o">.</span><span class="n">getList</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="n">name_list_1</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">name_list_2</span> <span class="k">if</span> <span class="s2">&quot;lr&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">x</span> <span class="p">]</span>
    <span class="n">name_list</span> <span class="o">=</span>  <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">name_list_1</span> <span class="k">if</span> <span class="s2">&quot;lowRes&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">x</span> <span class="p">]</span>
    <span class="k">return</span> <span class="n">name_list</span>
  
  <span class="k">def</span> <span class="nf">getPatchList</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">img_name</span><span class="p">):</span>
    <span class="s1">&#39;returns patch list of one image&#39;</span>
    <span class="c1">#get filenames and load images</span>
    <span class="n">filename</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dir</span> <span class="o">+</span> <span class="n">img_name</span>
    <span class="n">lr_img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">filename</span><span class="o">+</span><span class="s1">&#39;_lr.png&#39;</span><span class="p">),</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">)</span>

    <span class="c1">#parameters </span>
    <span class="n">stride_lr</span> <span class="o">=</span> <span class="n">x</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="mi">5</span><span class="o">%</span><span class="k">2</span>,3%2,3%2))
    <span class="n">tot_img_d</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">lr_img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">stride_lr</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">lr_img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">stride_lr</span><span class="p">)</span>    <span class="c1">#amount of image in height and width respectively</span>
    <span class="n">tot_img</span> <span class="o">=</span> <span class="n">tot_img_d</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">tot_img_d</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>     <span class="c1">#total amount of images</span>

    <span class="c1">#create list for current image</span>
    <span class="n">patch_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">tot_img_d</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">tot_img_d</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">patch_list</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">img_name</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">patch_list</span>
  
  <span class="k">def</span> <span class="nf">patchlist_get</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="s1">&#39;create patch_list&#39;</span>
    <span class="n">patch_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">namelist</span><span class="p">)):</span>
        <span class="n">patch_list</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">getPatchList</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">namelist</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Found&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">patch_list</span><span class="p">),</span> <span class="s1">&#39;trainable patches out of&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">namelist</span><span class="p">),</span> <span class="s1">&#39;images.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">patch_list</span>
  
  <span class="k">def</span> <span class="nf">createPatch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="s1">&#39;returns a patch&#39;</span>
    <span class="n">img_name</span> <span class="o">=</span> <span class="n">name</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">patch_name</span> <span class="o">=</span> <span class="n">name</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">name</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

    <span class="c1">#get corresponding images</span>
    <span class="n">filename</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dir</span> <span class="o">+</span> <span class="n">img_name</span>
    <span class="n">hr_img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">filename</span><span class="o">+</span><span class="s1">&#39;.png&#39;</span><span class="p">),</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2YCrCb</span><span class="p">)[:,:,</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># only get Y channel from YCrCb</span>
    <span class="n">lr_img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">filename</span><span class="o">+</span><span class="s1">&#39;_lr.png&#39;</span><span class="p">),</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2YCR_CB</span><span class="p">)[:,:,</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1">#create hr patch </span>
    <span class="n">stride_hr</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="mi">5</span><span class="o">%</span><span class="k">2</span>,3%2,3%2)))*r
    <span class="n">hr_patch</span> <span class="o">=</span> <span class="n">hr_img</span><span class="p">[</span><span class="n">stride_hr</span><span class="o">*</span><span class="n">patch_name</span><span class="p">[</span><span class="mi">0</span><span class="p">]:(</span><span class="n">stride_hr</span><span class="o">*</span><span class="n">patch_name</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="mi">17</span><span class="o">*</span><span class="n">r</span><span class="p">),</span><span class="n">stride_hr</span><span class="o">*</span><span class="n">patch_name</span><span class="p">[</span><span class="mi">1</span><span class="p">]:(</span><span class="n">stride_hr</span><span class="o">*</span><span class="n">patch_name</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mi">17</span><span class="o">*</span><span class="n">r</span><span class="p">)]</span>

    <span class="c1">#create lr patch </span>
    <span class="n">stride_lr</span> <span class="o">=</span> <span class="n">x</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="mi">5</span><span class="o">%</span><span class="k">2</span>,3%2,3%2))
    <span class="n">lr_patch</span> <span class="o">=</span> <span class="n">lr_img</span><span class="p">[</span><span class="n">stride_lr</span><span class="o">*</span><span class="n">patch_name</span><span class="p">[</span><span class="mi">0</span><span class="p">]:(</span><span class="n">stride_lr</span><span class="o">*</span><span class="n">patch_name</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="mi">17</span><span class="p">),</span><span class="n">stride_lr</span><span class="o">*</span><span class="n">patch_name</span><span class="p">[</span><span class="mi">1</span><span class="p">]:(</span><span class="n">stride_lr</span><span class="o">*</span><span class="n">patch_name</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mi">17</span><span class="p">)]</span>

    <span class="k">return</span> <span class="n">lr_patch</span><span class="p">,</span> <span class="n">hr_patch</span>

  <span class="k">def</span> <span class="nf">bicubic_upsampling</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">img_n</span><span class="p">):</span>
    <span class="s1">&#39;Loading high and low resolution image and do a bicubic upsampling of the low resolution image&#39;</span>
    <span class="c1">#get corresponding images</span>
    <span class="n">filename</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dir</span> <span class="o">+</span> <span class="n">img_n</span>
    <span class="n">hr_img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">filename</span> <span class="o">+</span> <span class="s1">&#39;.png&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s1">&#39;YCbCr&#39;</span><span class="p">)</span>
    <span class="n">lr_img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">filename</span><span class="p">[:</span><span class="o">-</span><span class="mi">9</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39;-lowRes.png&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s1">&#39;YCbCr&#39;</span><span class="p">)</span>

    <span class="c1">#upsample</span>
    <span class="n">bicubic</span> <span class="o">=</span> <span class="n">lr_img</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">hr_img</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">Image</span><span class="o">.</span><span class="n">BICUBIC</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">lr_img</span><span class="p">,</span> <span class="n">bicubic</span><span class="p">,</span> <span class="n">hr_img</span> <span class="c1"># return as pil images</span>

<span class="k">def</span> <span class="nf">getSlideList</span><span class="p">(</span><span class="n">slide_subfolders</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
  <span class="s1">&#39;Load Slide class for different directories&#39;</span>
  <span class="n">slides</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">slide</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">slide_subfolders</span><span class="p">):</span>
    <span class="n">slides</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Slide</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">slide</span><span class="p">,</span> <span class="n">i</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">slides</span>

<span class="n">slides</span> <span class="o">=</span> <span class="n">getSlideList</span><span class="p">(</span><span class="n">slide_subfolders</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="PSNR-calculation">PSNR calculation<a class="anchor-link" href="#PSNR-calculation"> </a></h3><p>PSNR is the performance metric to compare the real and the predicted high resolution images. The two functions below are used to calculate the mean squared error and the PSNR between two images. The PSNR calculation is already included here to make the code able to calculate the PSNR on validation images in between epochs during training.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">psnr_from_mselist</span><span class="p">(</span><span class="n">mse_list</span><span class="p">):</span>
  <span class="s1">&#39;Calculate PSNR&#39;</span>
  <span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">mse_list</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">mse</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="mi">20</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="mi">255</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mse</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">calc_mse</span><span class="p">(</span><span class="n">img_pred</span><span class="p">,</span> <span class="n">img_hr</span><span class="p">):</span>
  <span class="s1">&#39;Calculate MSE&#39;</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">img_pred</span><span class="o">*</span><span class="mi">255</span> <span class="o">-</span> <span class="n">img_hr</span><span class="o">*</span><span class="mi">255</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Dataset---Low-Resolution-image">Dataset - Low Resolution image<a class="anchor-link" href="#Dataset---Low-Resolution-image"> </a></h2><p>In the paper, the writers are mentioning their way of downsampling the images: <em>‘To synthesize the low-resolution samples, we blur the high-resolution images using a Gaussian filter and sub-sample them by the upscaling factor.’</em> [1] However, there is no mention of the size of the Gaussian blur. In this reproduction, there is chosen to set this kernel size of this blur to (5,5) as can be seen in the code below.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">## ONLY RUN THIS CELL IF LOW RESOLUTION IMAGES ARE NOT PRESENT IN THE DIRECTORY</span>
<span class="k">def</span> <span class="nf">createLowRes</span><span class="p">(</span><span class="n">img_name</span><span class="p">,</span> <span class="n">dir_91</span><span class="p">):</span>
  <span class="s1">&#39;saves low resolution image&#39;</span>
  <span class="c1"># Call HR image</span>
  <span class="n">filename</span> <span class="o">=</span> <span class="n">dir_91</span> <span class="o">+</span> <span class="n">img_name</span> <span class="o">+</span> <span class="s1">&#39;.png&#39;</span>
  <span class="n">hr_img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">filename</span><span class="p">),</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">)</span>

  <span class="c1"># Blur HR image</span>
  <span class="n">blur_img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">GaussianBlur</span><span class="p">(</span><span class="n">hr_img</span><span class="p">,(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span><span class="mi">0</span><span class="p">)</span>

  <span class="c1"># Apply subsampling</span>
  <span class="n">lr_img</span> <span class="o">=</span> <span class="n">blur_img</span><span class="p">[::</span><span class="n">r</span><span class="p">,::</span><span class="n">r</span><span class="p">]</span>

  <span class="c1"># Save lr_img</span>
  <span class="n">im</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">fromarray</span><span class="p">(</span><span class="n">lr_img</span><span class="p">)</span>
  <span class="n">im</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">dir_91</span> <span class="o">+</span> <span class="n">img_name</span> <span class="o">+</span> <span class="s1">&#39;_lr.png&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">slides</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">namelist</span><span class="p">)):</span>
  <span class="n">dir_91</span> <span class="o">=</span> <span class="n">path</span> <span class="o">+</span> <span class="s1">&#39;/CVPR2016_ESPCN_OurBenchMarkResult/Ours/&#39;</span> <span class="o">+</span> <span class="n">slide_subfolders</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">createLowRes</span><span class="p">(</span><span class="n">slides</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">namelist</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">dir_91</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />
<h2 id="Training">Training<a class="anchor-link" href="#Training"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Parameters">Parameters<a class="anchor-link" href="#Parameters"> </a></h3><p>The code below defines the settings for the training of the ESPCN. As described in the paper the mean squared error criterion and the Adam optimizer will be used. Furthermore, multiple training parameters and the scheduler for dynamic learning rate reduction are defined. An initial learning rate of 0.01 is used and a final learning rate of 1e-4 as described by the paper. The choice of other hyperparameters will be justified later.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Loss and optimizer</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>     
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">ConvNet</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>  

<span class="c1"># Parameters</span>
<span class="n">num_epoch</span> <span class="o">=</span> <span class="mi">5000</span>        <span class="c1">#Amount of epochs</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>         <span class="c1">#Batch size</span>
<span class="n">train_val_ratio</span> <span class="o">=</span> <span class="mf">0.95</span>  <span class="c1">#Training validation ratio</span>

<span class="c1"># Scheduler for dynamic reduction of the learning rate</span>
<span class="n">threshold_mu</span> <span class="o">=</span> <span class="mf">1e-6</span>     <span class="c1"># Treshold for decreasing learning rate.</span>
<span class="n">factor_value</span> <span class="o">=</span> <span class="mf">0.8</span>     <span class="c1"># Amount of decay per step, new lr = factor_value*lr.</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span>
                                                       <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span>
                                                       <span class="n">factor</span><span class="o">=</span><span class="n">factor_value</span><span class="p">,</span>
                                                       <span class="n">patience</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                                                       <span class="n">threshold</span><span class="o">=</span><span class="n">threshold_mu</span><span class="p">,</span>
                                                       <span class="n">min_lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span>
                                                       <span class="n">eps</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span>
                                                       <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />
<h3 id="Data-loader">Data loader<a class="anchor-link" href="#Data-loader"> </a></h3><p>In order to use the yang91 dataset during training, it is loaded into a Dataset class.  This class divides the list of available patches into different batches every epoch. Furthermore the returned patches, which as told before are created by the function <code>createPatch</code>,  are transformed into Torch tensors.</p>
<p>The Dataset class is divided into two parts. One part contains 95% of the available training patches, while the second part contains the remaining 5%. The two parts are both loaded into a Dataloader. The Dataloader which has 95% of the training patches is called <code>training_generator</code> and is used to provide the model with patches during training of the model. The other Dataloader is called <code>validation_generator</code> and is called to validate the model during training time.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">DataGenerator</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="p">):</span>
  <span class="s1">&#39;Generates the dataset that is used for training the ESPCN&#39;</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">slides</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">slides</span> <span class="o">=</span> <span class="n">slides</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transform</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
        <span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()])</span>

  <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="s1">&#39;Returns the amount of patches&#39;</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">slides</span><span class="o">.</span><span class="n">patchlist</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
    <span class="s1">&#39;Returns low and high resolution patches in the form of tensors&#39;</span>
    <span class="n">lr_patch</span><span class="p">,</span> <span class="n">hr_patch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">slides</span><span class="o">.</span><span class="n">createPatch</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">slides</span><span class="o">.</span><span class="n">patchlist</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
    
    <span class="c1"># transform images to pytorch tensors</span>
    <span class="n">lr_patch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">lr_patch</span><span class="p">)</span>
    <span class="n">hr_patch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">hr_patch</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">lr_patch</span><span class="p">,</span> <span class="n">hr_patch</span>   

<span class="c1"># Put DataGenerator in DataLoader</span>
<span class="n">full_dataset</span> <span class="o">=</span> <span class="n">DataGenerator</span><span class="p">(</span><span class="n">slides</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># Split between training and validation set</span>
<span class="n">train_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">train_val_ratio</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">full_dataset</span><span class="p">))</span>
<span class="n">validation_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">full_dataset</span><span class="p">)</span> <span class="o">-</span> <span class="n">train_size</span>
<span class="n">training_set</span><span class="p">,</span> <span class="n">validation_set</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">random_split</span><span class="p">(</span><span class="n">full_dataset</span><span class="p">,</span> <span class="p">[</span><span class="n">train_size</span><span class="p">,</span> <span class="n">validation_size</span><span class="p">])</span>

<span class="c1"># Create training and validation data loaders</span>
<span class="n">training_generator</span>      <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">training_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="s1">&#39;True&#39;</span><span class="p">)</span>
<span class="n">validation_generator</span>    <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">validation_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="s1">&#39;False&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Training-the-model">Training the model<a class="anchor-link" href="#Training-the-model"> </a></h3><p>The actual training of the model is done in the code below. Every epoch consists of two parts. In the first part (<em>training</em>), the model is trained and in the second part (<em>validation</em>) the model is validated. The algorithm will repeat these two parts until it has done the number of epochs defined above.
During training low resolution patches are sent in batches to the GPU. On the GPU, the forward pass of the ESPCN network is executed. Next, the corresponding high resolution patches and the outputs after the forward pass are used to calculate the loss. During the backpropagation, the optimizer is used to find the gradient for every parameter. The parameters are updated correspondingly to that gradient. At last, the average loss of one epoch is stored in a list.
During validation the same procedure as in training is followed, with the exception of not executing the backpropagation. Furthermore, the PSNR of the validation patches is calculated and stored. 
After the training, it will compute the training time.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Initializing lists used for saving losses and PSNR</span>
<span class="n">loss_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">epoch_loss_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">val_loss_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">validation_psnr</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Start timer</span>
<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

<span class="c1"># Training loop</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epoch</span><span class="p">):</span>
  <span class="c1"># Switch to training mode</span>
  <span class="n">ConvNet</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">lr_patch</span><span class="p">,</span> <span class="n">hr_patch</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">training_generator</span><span class="p">):</span>
      
    <span class="c1"># Transfer training data to active device</span>
    <span class="n">lr_patch</span><span class="p">,</span> <span class="n">hr_patch</span> <span class="o">=</span> <span class="n">lr_patch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">hr_patch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    
    <span class="c1"># Run the forward pass</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">ConvNet</span><span class="p">(</span><span class="n">lr_patch</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">hr_patch</span><span class="p">)</span>
    <span class="n">loss_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="c1"># Backprop and perform Adam optimisation</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

  <span class="c1"># Save loss every epoch</span>
  <span class="n">epoch_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">loss_list</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">training_generator</span><span class="p">)</span>
  <span class="n">epoch_loss_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch_loss</span><span class="p">)</span>

  <span class="c1"># Print epoch loss every 50 epochs</span>
  <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Epoch&quot;</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="s2">&quot;loss: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch_loss</span><span class="p">))</span>
  
  <span class="c1"># Save model every 1000 epochs (in case Google Colab stops runtime)</span>
  <span class="c1"># if epoch % 1000 == 999:</span>
  <span class="c1">#   model_name = &#39;final_&#39; + str(epoch+1) + &#39;_epochs&#39;</span>
  <span class="c1">#   path_model = path + &#39;/saved_models/&#39; + model_name</span>
  <span class="c1">#   torch.save(ConvNet.state_dict(), path_model)</span>
  <span class="c1">#   print(&#39;Model saved as: &#39;, model_name)</span>

  <span class="c1"># Step to next step of lr-scheduler</span>
  <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">epoch_loss</span><span class="p">)</span>
  <span class="n">loss_list</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="c1"># Enter validation mode</span>
  <span class="n">ConvNet</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

  <span class="c1"># Keep track of mse for every patch, to collectively calculate PSNR per epoch</span>
  <span class="n">epoch_mse</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">lr_patch</span><span class="p">,</span> <span class="n">hr_patch</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">validation_generator</span><span class="p">):</span>
      <span class="c1"># Transfer training data to active device (GPU)</span>
      <span class="n">lr_patch</span><span class="p">,</span> <span class="n">hr_patch</span> <span class="o">=</span> <span class="n">lr_patch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">hr_patch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

      <span class="c1"># Predict output</span>
      <span class="n">img_pred</span> <span class="o">=</span> <span class="n">ConvNet</span><span class="p">(</span><span class="n">lr_patch</span><span class="p">)</span>
      
      <span class="c1"># Calculate validation loss</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">img_pred</span><span class="p">,</span> <span class="n">hr_patch</span><span class="p">)</span>
      <span class="n">loss_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
      
      <span class="c1"># Calculate mse for every sample </span>
      <span class="n">img_pred</span> <span class="o">=</span> <span class="n">img_pred</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
      <span class="n">hr_patch</span> <span class="o">=</span> <span class="n">hr_patch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
      <span class="n">epoch_mse</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">calc_mse</span><span class="p">(</span><span class="n">img_pred</span><span class="p">,</span> <span class="n">hr_patch</span><span class="p">))</span>

    <span class="c1"># Calculate validation psnr on the complete epoch from all individual MSE&#39;s</span>
    <span class="n">val_psnr</span> <span class="o">=</span> <span class="n">psnr_from_mselist</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">epoch_mse</span><span class="p">))</span>
    <span class="n">validation_psnr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val_psnr</span><span class="p">)</span>
  
  <span class="c1"># Save validation loss every epoch</span>
  <span class="n">val_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">loss_list</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">validation_generator</span><span class="p">)</span>
  <span class="n">val_loss_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val_loss</span><span class="p">)</span>
  
  <span class="n">loss_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Training took </span><span class="si">{}</span><span class="s1"> seconds&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Seconds per epoch:&#39;</span><span class="p">,(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">t0</span><span class="p">)</span><span class="o">/</span><span class="n">num_epoch</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Plotting-of-training-results">Plotting of training results<a class="anchor-link" href="#Plotting-of-training-results"> </a></h3><p>The script below will plot the training and validation loss for each epoch and a separate plot for the validation PSNR per epoch.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Show training and validation loss of current model in memory</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Loss per epoch&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_epoch</span><span class="p">),</span> <span class="n">epoch_loss_list</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_epoch</span><span class="p">),</span> <span class="n">val_loss_list</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;PSNR for validation data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_epoch</span><span class="p">),</span> <span class="n">validation_psnr</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Testing">Testing<a class="anchor-link" href="#Testing"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Loading-the-trained-model">Loading the trained model<a class="anchor-link" href="#Loading-the-trained-model"> </a></h3><p>The code below is used to load the parameters of previous trained models from the ‘saved_models’ directory.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Load weight in earlier defined model</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;final_test_5000_epochsweights&#39;</span>
<span class="n">path_model</span> <span class="o">=</span> <span class="n">path</span> <span class="o">+</span> <span class="s1">&#39;/saved_models/&#39;</span> <span class="o">+</span> <span class="n">model_name</span>
<span class="n">ConvNet</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">path_model</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Test-image-plot-function">Test image plot function<a class="anchor-link" href="#Test-image-plot-function"> </a></h3><p>The function below is used to compare the predictions with the low and high resolution images. From left to right it shows the low resolution, the output of the neural network and the high resolution image respectively.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">generate_figure</span><span class="p">(</span><span class="n">lr_img</span><span class="p">,</span> <span class="n">sr_img</span><span class="p">,</span> <span class="n">hr_img</span><span class="p">):</span>
  <span class="s1">&#39;Show the low resolution, upscaled and high resolution image&#39;</span>
  <span class="n">f</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
  <span class="n">f</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToPILImage</span><span class="p">(</span><span class="s1">&#39;YCbCr&#39;</span><span class="p">)(</span><span class="n">lr_img</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s1">&#39;RGB&#39;</span><span class="p">))</span>
  <span class="n">f</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToPILImage</span><span class="p">(</span><span class="s1">&#39;YCbCr&#39;</span><span class="p">)(</span><span class="n">sr_img</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s1">&#39;RGB&#39;</span><span class="p">))</span>
  <span class="n">f</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToPILImage</span><span class="p">(</span><span class="s1">&#39;YCbCr&#39;</span><span class="p">)(</span><span class="n">hr_img</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s1">&#39;RGB&#39;</span><span class="p">))</span> 
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Data-loader">Data loader<a class="anchor-link" href="#Data-loader"> </a></h3><p>The Dataloader in the next code snippet is the same as the used Dataloader for training. However, it deviates from the training Dataloader in two ways. Firstly, it loads images, instead of patches (during training). Secondly, it also loads bicubic upsampled low resolution images. In the next section, it will be explained why the bicubic upsampled images are needed.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">DataGenerator_test</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="p">):</span>
  <span class="s1">&#39;Generates the dataset used for testing&#39;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">slides</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">slides</span> <span class="o">=</span> <span class="n">slides</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transform</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()])</span>

  <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="s1">&#39;Returns the amount of test images&#39;</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">slides</span><span class="o">.</span><span class="n">namelist</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
    <span class="s1">&#39;Returns low, upsampled and high resolution testing images in the form of tensors&#39;</span>
    <span class="n">lr_img</span><span class="p">,</span> <span class="n">bicubic</span><span class="p">,</span> <span class="n">hr_img</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">slides</span><span class="o">.</span><span class="n">bicubic_upsampling</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">slides</span><span class="o">.</span><span class="n">namelist</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
    
    <span class="n">lr_img</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">lr_img</span><span class="p">)</span>
    <span class="n">hr_img</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">hr_img</span><span class="p">)</span>
    <span class="n">bicubic</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">bicubic</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">lr_img</span><span class="p">,</span> <span class="n">bicubic</span><span class="p">,</span> <span class="n">hr_img</span>

<span class="c1">## Put DataGenerator in DataLoader</span>
<span class="k">def</span> <span class="nf">DataGenerator</span><span class="p">(</span><span class="n">slides</span><span class="p">):</span>
  <span class="s1">&#39;Create dataloader for every test directory&#39;</span>
  <span class="n">test_set</span> <span class="o">=</span> <span class="n">DataGenerator_test</span><span class="p">(</span><span class="n">slides</span><span class="p">)</span>
  <span class="n">test_generator</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">test_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">test_generator</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Testing-the-model">Testing the model<a class="anchor-link" href="#Testing-the-model"> </a></h3><p>Finally, the ESPCN can be tested on the different test data sets. In the code below Set5, Set14, BSD300, BSD500 and the Supertexture136 dataset are used to reproduce the results from the paper.</p>
<p>The low resolution images are fed to the ESPCN, which outputs only the predicted high resolution Y (luminance) channel of the input images. Next, the Y channels of the bicubic upsampled images are replaced by these predicted Y channels of the network. This last step is done to display a random sample of every test set.</p>
<p>In order to compare this reproduction with the paper, the PSNR of every test set is calculated. Also, the average time it takes to produce an upsampled image by this reproduction is determined. This is computed to get an intuition about the authors claim of real-time SR of videos.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">random</span> <span class="k">as</span> <span class="nn">rnd</span>
<span class="n">rnd</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">9</span><span class="p">)</span>

<span class="c1"># Loop over test data sets</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">slides</span><span class="p">[</span><span class="mi">1</span><span class="p">:])):</span>

  <span class="c1"># Load data</span>
  <span class="n">test_generator</span> <span class="o">=</span> <span class="n">DataGenerator</span><span class="p">(</span><span class="n">slides</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>

  <span class="n">show</span> <span class="o">=</span> <span class="n">rnd</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">test_generator</span><span class="p">))</span>

  <span class="c1"># Define lists</span>
  <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">mse_list</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">time_list</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="c1"># Main testing loop</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">lr_img</span><span class="p">,</span> <span class="n">bicubic</span><span class="p">,</span> <span class="n">hr_img</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_generator</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        
      <span class="c1"># Only test on the Y (intensity channel)</span>
      <span class="n">to_network</span> <span class="o">=</span> <span class="p">(</span><span class="n">lr_img</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,:,:])</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

      <span class="c1"># Start timer</span>
      <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
      
      <span class="c1"># Run the forward pass</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">ConvNet</span><span class="o">.</span><span class="n">cpu</span><span class="p">()(</span><span class="n">to_network</span><span class="p">)</span>
      <span class="n">img_pred</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

      <span class="c1"># Substitute the Y channel from bicubic with the one outputted by the model</span>
      <span class="n">bicubic</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,:,:]</span> <span class="o">=</span> <span class="n">img_pred</span>

      <span class="c1">#Stop timer</span>
      <span class="n">elapsed_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
      <span class="n">time_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">elapsed_time</span><span class="p">)</span>

      <span class="c1"># Calculate MSE</span>
      <span class="n">mse</span> <span class="o">=</span> <span class="n">calc_mse</span><span class="p">(</span><span class="n">img_pred</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">hr_img</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[:,</span><span class="mi">0</span><span class="p">,:,:])</span>
      <span class="n">mse_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mse</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

      <span class="c1">#Show one random image from every dataset</span>
      <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">show</span><span class="p">:</span>
        <span class="n">generate_figure</span><span class="p">(</span><span class="n">lr_img</span><span class="p">,</span> <span class="n">bicubic</span><span class="p">,</span> <span class="n">hr_img</span><span class="p">)</span>

  <span class="c1"># Printing the results</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">slide_subfolders</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;: PSNR&#39;</span><span class="p">,</span> <span class="n">psnr_from_mselist</span><span class="p">(</span><span class="n">mse_list</span><span class="p">),</span> <span class="s1">&#39;Average time&#39;</span> <span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">time_list</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Hyperparameter-tuning">Hyperparameter tuning<a class="anchor-link" href="#Hyperparameter-tuning"> </a></h1><p>As described above there are multiple uncertainties regarding the reproducibility of the paper. Therefore, hyperparameter tuning is conducted on multiple hyperparameters in order to match the results of the paper. The hyperparameters which will have been tuned are the batch size, the learning rate threshold μ, the learning rate decay factor, the learning rate patience and the ratio between the training and the validation set. All hyperparameters are tuned independently  while keeping the other hyperparameters fixed during training sessions of 300 epochs. Results of the hyperparameter tuning can be seen in the separate notebook which can be found <a href="https://github.com/sfalkena/ESPCN_reproduction/tree/master/_notebooks/Hyperparameter_tuning">here</a>. The default values of the hyperparameters have been set to:</p>
<ul>
<li>Batch size: 16</li>
<li>Learning rate threshold μ: 1e-4</li>
<li>Learning rate decay factor: 0.5</li>
<li>Learning rate patience: 2</li>
<li>Training validation set ratio: 80/20</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Batch-size">Batch size<a class="anchor-link" href="#Batch-size"> </a></h2><p>Batch size is one of the hyperparameters not specified in the paper which can have an impact on the final results. Having a small batch size will increase the training speed and needs less memory but it will reduce the accuracy of gradient estimation. For optimization a batch size of 4, 8 ,16 and 32 is investigated. As it can be seen below a batch size of 16 will result in the highest performance during training. Furthermore, a strange behaviour is observed for a batch size of 4 (green line). At first, the loss decays normally, after which it suddenly explodes and decreases again. There needs to be done more research into why this is happening. The defined learning rate decay and/or Adam optimizer might be the cause of this behaviour.</p>
<p><img src="https://drive.google.com/uc?id=13z1dtWC8zcj5_yRGzS-Nwi2jYy4BjHTx" alt="hp_tuning_batchsize" /></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Learning-rate-threshold-&#956;">Learning rate threshold &#956;<a class="anchor-link" href="#Learning-rate-threshold-&#956;"> </a></h2><p>The learning rate threshold μ is described as a threshold on the improvement of the cost function. If the improvement of the cost function is smaller than the learning threshold then the learning rate will be decreased. With a higher μ the learning rate will reach the final learning rate of 0.0001 earlier during training. In order to establish the μ which gives the best results a threshold of 1e-2, 1e-4, 1e-6, 1e-8 and 1e-10 is investigated. As can be seen, every threshold results in almost the same performance, with the exception of the largest threshold: 1e-2. The values 1e-6 and 1e-10 appear to give the best results, however the margin with the other values is small. Therefore, this slightly better result can be due to the randomness in the learning process (initialization of model, splitting training dataset into training and validation patches, batch generation). The experiment needs to be repeated to give a standard deviation to the curves, and thus a better answer to the question which learning rate needs to be used. 
<img src="https://drive.google.com/uc?id=1S8PQrXGckJ9_J7It6CbR7Iby10rmsnZO" alt="hp_tuning_mu" /></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Learning-rate-decay-factor">Learning rate decay factor<a class="anchor-link" href="#Learning-rate-decay-factor"> </a></h2><p>Whenever the learning rate threshold μ is reached the learning rate will decrease with a certain factor. This factor is described by the learning rate decay factor and is also not specified in the paper. In contrast to the learning rate threshold μ, a higher learning rate decay factor will result in the final learning rate being reached later during training. To find the learning rate decay which results in the highest performance the model is trained on a learning rate decay factor of 0.5, 0.6, 0.7, 0.8 and 0.9. As it can be seen below a learning rate decay factor of 0.8 will result in the highest performance during training.</p>
<p><img src="https://drive.google.com/uc?id=1ocxHqT-hOncwyv4mJknyNvx-B2f_HBVa" alt="hp_tuning_factor" /></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Learning-rate-patience">Learning rate patience<a class="anchor-link" href="#Learning-rate-patience"> </a></h2><p>Another hyperparameter which is tunable is the learning rate patience. Whenever the learning rate threshold μ is exceeded, the model will delay the decreasing of the learning rate with a number of epochs. This delay is defined by the learning rate patience. For optimization a learning rate patience of 1, 2, 4, and 8 are investigated. As it can be seen below, a learning rate patience of 8 gives significantly lower performance compared to the others which are quite similar.</p>
<p><img src="https://drive.google.com/uc?id=1sE7fNnvUTj7mb945Re28WzkfyOy1vx1G" alt="hp_tuning_patience" /></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Ratio-between-the-training-and-the-validation-set">Ratio between the training and the validation set<a class="anchor-link" href="#Ratio-between-the-training-and-the-validation-set"> </a></h2><p>The final hyperparameter which is tuned is the ratio between the size of the training set and the size of the validation set. From the yang91 training set a small subset is subsampled to validate the performance during training. For optimization usage of 80%, 90% and 95% of the training set are investigated. As it can be seen below a training validation ratio of 95/5 will result in the best performances. This is as expected, as the model has now learned on more patches per epoch and will thus be able to perform better on patches of the same types of images. 
<img src="https://drive.google.com/uc?id=1IfL2iXUCW6Cf2Fz1Fh01mDT_IngwZNVj" alt="hp_tuning_ratio" /></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Results">Results<a class="anchor-link" href="#Results"> </a></h1><h2 id="Chosen-hyperparameters">Chosen hyperparameters<a class="anchor-link" href="#Chosen-hyperparameters"> </a></h2><p>After the hyperparameter tuning, there is chosen to set the hyperparameters to the following values for the final training run:</p>
<ul>
<li>Batch size: 16</li>
<li>Learning rate threshold μ: 1e-6</li>
<li>Learning rate decay factor: 0.8</li>
<li>Learning rate patience: 1</li>
<li>Training validation set ratio: 95/5</li>
<li>Number of epochs: 5000</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Training-results">Training results<a class="anchor-link" href="#Training-results"> </a></h2><p>In the figures below, one can see the PSNR of the validation data vs. the number of epochs and the training and validation loss vs number of epochs. It can be seen that the model is learning, also for a large number of epochs. However, the amount it learns is decreasing for an increasing amount of epochs.</p>
<p><img src="https://drive.google.com/uc?id=1llCHlF0uBqwogKOAkFGw4zEKzNbHKcxk" alt="training_results" /></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Test-set-results">Test set results<a class="anchor-link" href="#Test-set-results"> </a></h2><p>The reproduced model has been evaluated on the 5 test datasets as described earlier. The results are shown in the table below. Values are in dB and represent the mean PSNR of that dataset.</p>
<table>
<thead><tr>
<th>Dataset:</th>
<th>Set5</th>
<th>Set14</th>
<th>BSD300</th>
<th>BSD500</th>
<th>SuperTexture136</th>
</tr>
</thead>
<tbody>
<tr>
<td>Results by [1], using tanh</td>
<td>32.55</td>
<td>29.08</td>
<td>28.26</td>
<td>28.34</td>
<td>26.42</td>
</tr>
<tr>
<td>Reproduced results</td>
<td>24.15</td>
<td>24.10</td>
<td>25.29</td>
<td>25.16</td>
<td>24.40</td>
</tr>
</tbody>
</table>
<p>Below you can see three images of a butterfly. The left image is the low resolution image, the middle image is the image produced by the model and the right the high resolution image.</p>
<p><img src="https://drive.google.com/uc?id=1zIz90RDIyQa8vPAbY0l8jhiWORZlLnyo" alt="result_butterfly" /></p>
<p>As can be clearly seen, the model is in general capable of upsampling the low resolution image to a high resolution image. However, when looking closely at some white spots on the butterfly’s wing, one can see black spots. Here, the model fails locally to produce a good image. These black spots will be discussed in the next section.</p>
<p>Again you can see three images below, this time from peppers (left: low resolution, middle: predicted high resolution, right: high resolution).</p>
<p><img src="https://drive.google.com/uc?id=16nrdyu84zEyFoB5CysEq7CsZSwtnXelB" alt="result_peppers" /></p>
<p>Again, the prediction looks to be very similar to the high resolution image. However, there appear some white spots in black areas (right bottom of image). These white spots will also be discussed in the next section.</p>
<p>In order to question the claim of the writers of the papers that real-time SR of 1080p videos is possible with their model, we calculated the average time it takes to produce an image by our reproduction. The obtained values in seconds are shown in the table below.</p>
<table>
<thead><tr>
<th>Dataset:</th>
<th>Set5</th>
<th>Set14</th>
<th>BSD300</th>
<th>BSD500</th>
<th>SuperTexture136</th>
</tr>
</thead>
<tbody>
<tr>
<td>Time per image[s]</td>
<td>0.0158</td>
<td>0.0303</td>
<td>0.0199</td>
<td>0.0200</td>
<td>0.0114</td>
</tr>
</tbody>
</table>
<p>A standard video has a frame rate of 27 FPS. [5] This means that the model needs to produce an image within (1/27) 0.0370s. As can be seen in the table, this is indeed the case. However, these images are not 1080p, but more close to 180p. However, we think that the table is an indication that the model should be able to do real-time SR of 1080p videos, as the above results are obtained on a CPU of google colab. Running the model on a GPU on a good pc nowadays will most likely produce 1080p images even faster.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Discussion">Discussion<a class="anchor-link" href="#Discussion"> </a></h1><p>Here, the reproducibility of the paper will be discussed. First, let's discuss a couple of things that were unclear about the implementation:</p>
<h2 id="Unclarities">Unclarities<a class="anchor-link" href="#Unclarities"> </a></h2><h3 id="The-stopping-criterion">The stopping criterion<a class="anchor-link" href="#The-stopping-criterion"> </a></h3><p>In the paper, the authors define the stopping criterion as follows: The training stops after no improvement of the cost function is observed after 100 epochs. However, only this sentence can already be interpreted in two ways: Train for a minimum of 100 epochs and then stop when no improvement can be observed, or, train for a number of epochs, so that a window of 100 epochs falls within some threshold value. Moreover, the value of this threshold is not mentioned in the paper. As this would require another hyperparameter to tune, it has been chosen to train the network for a fixed number of epochs, so that the network trains within some time limit. No indication on the total number of epochs has been given in the paper, so it has been set to a value of 300 during most of the hyperparameter tuning. Within 300 epochs, the loss converges enough to see if a setting works well or not. For the final model, the amount of epochs was based on available time and has been chosen to be 5000 epochs</p>
<h3 id="The-learning-rate">The learning rate<a class="anchor-link" href="#The-learning-rate"> </a></h3><p>The paper mentions the following about the value of the learning rate: Initial learning rate is set to 0.01 and final learning rate is set to 0.0001 and updated gradually when the improvement of the cost function is smaller than a threshold µ. This has been implemented with the PyTorch method: ReduceLRonPlateau, which reduces the learning rate in a similar way as described in the paper. However, the value of µ has not been mentioned in the paper. During hyperparameter tuning, several values have been tried, leading to the conclusion that only large values of µ were not working very well.</p>
<h3 id="The-luminance-channel">The luminance channel<a class="anchor-link" href="#The-luminance-channel"> </a></h3><p>The paper states the following sentence: “For our final models, we use 50,000 randomly selected images from ImageNet [5] for the training. Following previous works, we only consider the luminance channel in YCbCr colour space in this section because humans are more sensitive to luminance changes” [6]. This sentence has been interpreted as that the model has been trained and evaluated on the luminance (Y) channel of the input images. This conclusion has been drawn by also looking at the model architecture where it seems that only one channel has been used as the input for the model. As the model also outputs the luminance channel, the choice has been made to also evaluate the model on the PSNR of two luminance images.</p>
<h2 id="Discussion-of-our-implementation:">Discussion of our implementation:<a class="anchor-link" href="#Discussion-of-our-implementation:"> </a></h2><p>The unclarities about the paper are not the only thing contributing to the fact that our results are different from the original paper. During the visualization of the test phase, it was noted that some of the output images contained white spots in dark areas and dark spots in light areas. We believe this is a great contributor to our apparently low PSNR. We have been trying to understand what causes this, but at the time of writing, have not succeeded in finding the cause.</p>
<p>Furthermore, we found some inconsistencies in the used test data. The high resolution images of one folder appear to be slightly different than the high resolution images of another folder, while this should in fact be exactly the same images. Therefore, it can be the case that we have used slightly altered test images in comparison with the paper’s test images, resulting in lower PSNR values.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="To-reproduce-or-not-to-reproduce?">To reproduce or not to reproduce?<a class="anchor-link" href="#To-reproduce-or-not-to-reproduce?"> </a></h1><p>The aim of this blogpost is to reproduce the results found in the “Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network”. In order to do so, a pytorch implementation is written from scratch using only information from the paper itself. The paper presents a method for image super resolution which is fast enough so that it can be applied on real-time video material. This is done with a CNN architecture, consisting of three convolutional layers, where the feature maps are extracted on the luminance channel in low resolution space. After this, the resolution is increased only at the very end by an upsampling layer.</p>
<p>At the beginning of the project the paper was easy to read for us being no experts in the field of deep learning. Diving into the process of reproducing the paper we discovered that some hyperparameters for training were not specified in the paper. Also, we encountered some other uncertainties related to the stopping criterion and the luminance channel. Apart from the appearance of black and white spots we believe the visual results of our reproduction attempt are considerably accurate.</p>
<p>Although the performance of our reproduction is not as high as stated in the paper we believe that it is reproducible. As this was our first project in the field of deep learning we had no experience in finding the correct hyperparameters for training. For someone with a bit more experience we think the results of the paper should be easily reproducible.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="References">References<a class="anchor-link" href="#References"> </a></h1><p>[1] W. Shi et al., “Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016, doi: 10.1109/cvpr.2016.207.</p>
<p>[2] Y. Chen and T. Pock, “Trainable Nonlinear Reaction Diffusion: A Flexible Framework for Fast and Effective Image Restoration,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 39, no. 6, pp. 1256–1272, Jun. 2017.</p>
<p>[3] C. Dong, C. C. Loy, K. He, and X. Tang, “Image Super-Resolution Using Deep Convolutional Networks,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 38, no. 2. pp. 295–307, 2016, doi: 10.1109/tpami.2015.2439281.</p>
<p>[4] C. Osendorfer, H. Soyer, and P. van der Smagt, “Image Super-Resolution with Fast Approximate Convolutional Sparse Coding,” Neural Information Processing. pp. 250–257, 2014, doi: 10.1007/978-3-319-12643-2_31.</p>
<p>[5] R. Taylor, “A Beginners Guide to Frame Rates : Aframe.” [Online]. Available: <a href="https://aframe.com/blog/2013/07/a-beginners-guide-to-frame-rates/">https://aframe.com/blog/2013/07/a-beginners-guide-to-frame-rates/</a>. [Accessed: 20-Apr-2020].</p>
<p>[6] O. Russakovsky et al., “ImageNet Large Scale Visual Recognition Challenge,” International Journal of Computer Vision, vol. 115, no. 3. pp. 211–252, 2015, doi: 10.1007/s11263-015-0816-y.</p>
<p>[7] S. Schulter, C. Leistner, and H. Bischof, “Fast and accurate image upscaling with super-resolution forests,” 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2015, doi: 10.1109/cvpr.2015.7299003.</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="sfalkena/ESPCN_reproduction"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/ESPCN_reproduction/fastpages/jupyter/2020/04/20/ESPCN_reproduction.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/ESPCN_reproduction/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/ESPCN_reproduction/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/ESPCN_reproduction/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Where I will keep my blogs of projects that I did.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/ESPCN_reproduction/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/ESPCN_reproduction/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
