{
  
    
        "post0": {
            "title": "Title",
            "content": "# &quot;Reproduction of &#39;Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network&#39; &quot; &gt; &quot;This blog post describes the reproduction of the paper: &#39;Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network&#39;. It explains the main point of the paper, tries to reproduce the results of table 1 of the paper and argues about it&#39;s reproducibility&quot; - toc: true- branch: master- badges: true - comments: true - author: Luuk Balkenende &amp; Sieger Falkena &amp; Luc Kloosterlaan - categories: [fastpages, jupyter] . . Running the experiment on Google Colab . This notebook is running remotely on the Google Colab platform. Therefore, to save and access the trained model, we needed to mount the Google drive. We used the following code snippet to set up a local drive on our computer. . from torchvision import transforms from google.colab import drive from torch.utils import data from PIL import Image import matplotlib.pyplot as plt import PIL.Image as pil_image import torch.nn as nn import random as rnd import torchvision import numpy as np import torch import time import cv2 import os . drive.mount(&#39;/content/gdrive&#39;) path =&#39;/content/gdrive/My Drive/deep_learning_group_7&#39; os.chdir(path) . Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&amp;redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&amp;response_type=code&amp;scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly Enter your authorization code: ·········· Mounted at /content/gdrive . from IPython.display import HTML HTML(filename=path+&quot;/introductory_notebooks/interactive_image.html&quot;) . &lt;!DOCTYPE html&gt; . Comparison of low resolution and upscaled image . Click and slide the blue slider to compare two images: . . ESPCN Architecture . Below, you can find the ESPCN architecture as defined in the paper. Notice that we have used tanh as activation function, as the writers indicated that this leads to better results. . Most important to notice here is that the sub-pixel convolution layer is divided into two layers: one normal convolutional layer (conv3) and a layer which preforms the SR operation (upsample). . r = 3 #scaling factor class SuperResConvNet(nn.Module): def __init__(self): super(SuperResConvNet, self).__init__() self.conv1 = nn.Conv2d(1,64, kernel_size=5, stride=1, padding=2) self.conv2 = nn.Conv2d(64,32, kernel_size=3, stride=1, padding=1) self.conv3 = nn.Conv2d(32,1*r**2, kernel_size=3, stride=1, padding=1) self.upsample = nn.PixelShuffle(r) def forward(self, y): y = torch.tanh(self.conv1(y)) y = torch.tanh(self.conv2(y)) y = self.conv3(y) y = self.upsample(y) return y if torch.cuda.is_available(): print(&quot;Using GPU&quot;) device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) ConvNet = SuperResConvNet() ConvNet.to(device) . Functions . slide_subfolders = [&#39;/CVPR2016_ESPCN_OurBenchMarkResult/Ours/yang91/T91/&#39;, &#39;/CVPR2016_ESPCN_OurBenchMarkResult/Ours/x3.0/Set5/&#39;, &#39;/CVPR2016_ESPCN_OurBenchMarkResult/Ours/x3.0/Set14/&#39;, &#39;/CVPR2016_ESPCN_OurBenchMarkResult/Ours/x3.0/BSD300/&#39;, &#39;/CVPR2016_ESPCN_OurBenchMarkResult/Ours/x3.0/BSD500/&#39;, &#39;/CVPR2016_ESPCN_OurBenchMarkResult/Ours/x3.0/SuperText136/&#39;] #width and height of patches x = 17 class Slide: def __init__(self, path, slide_subfolders, count): self.count = count self.dir = path + slide_subfolders self.namelist = self.deleteLRImage() if count == 0: self.patchlist = self.patchlist_get() def removePng(f): &#39;returns filename without png&#39; filename_parts = f[:-4] return filename_parts def getList(self): &#39;returns list of filenames&#39; return [Slide.removePng(f) for f in os.listdir(self.dir) if f.endswith(&quot;.png&quot;)] def deleteLRImage(self): &#39;deletes LR images from list if they are already made (which they are)&#39; name_list_2 = Slide.getList(self) name_list_1 = [x for x in name_list_2 if &quot;lr&quot; not in x ] name_list = [x for x in name_list_1 if &quot;lowRes&quot; not in x ] return name_list def getPatchList(self, img_name): &#39;returns patch list of one image&#39; #get filenames and load images filename = self.dir + img_name lr_img = cv2.cvtColor(cv2.imread(filename+&#39;_lr.png&#39;), cv2.COLOR_BGR2RGB) #parameters stride_lr = x-np.sum((5%2,3%2,3%2)) tot_img_d = int(lr_img.shape[0]/stride_lr), int(lr_img.shape[1]/stride_lr) #amount of image in height and width respectively tot_img = tot_img_d[0]*tot_img_d[1] #total amount of images #create list for current image patch_list = [] for i in range(tot_img_d[0]-1): for j in range(tot_img_d[1]-1): patch_list.append([img_name, i,j]) return patch_list def patchlist_get(self): #create patch_list patch_list = [] for i in range(len(self.namelist)): patch_list.extend(self.getPatchList(self.namelist[i])) print(&#39;Found&#39;, len(patch_list), &#39;trainable patches out of&#39;, len(self.namelist), &#39;images.&#39;) return patch_list def createPatch(self, name): &#39;returns a patch&#39; img_name = name[0] patch_name = name[1], name[2] #get corresponding images filename = self.dir + img_name hr_img = cv2.cvtColor(cv2.imread(filename+&#39;.png&#39;), cv2.COLOR_BGR2YCrCb)[:,:,0] # only get Y channel from YCrCb lr_img = cv2.cvtColor(cv2.imread(filename+&#39;_lr.png&#39;), cv2.COLOR_BGR2YCR_CB)[:,:,0] #create hr patch stride_hr = (x-np.sum((5%2,3%2,3%2)))*r hr_patch = hr_img[stride_hr*patch_name[0]:(stride_hr*patch_name[0]+17*r),stride_hr*patch_name[1]:(stride_hr*patch_name[1]+17*r)] #create lr patch stride_lr = x-np.sum((5%2,3%2,3%2)) lr_patch = lr_img[stride_lr*patch_name[0]:(stride_lr*patch_name[0]+17),stride_lr*patch_name[1]:(stride_lr*patch_name[1]+17)] return lr_patch, hr_patch def image_operations_testing(self, img_n): #get corresponding images hr_img = cv2.cvtColor(cv2.imread(self.dir + img_n + &#39;.png&#39;), cv2.COLOR_BGR2YCrCb)[:,:,0] lr_img = cv2.cvtColor(cv2.imread(self.dir + img_n[:-9] + &#39;-lowRes.png&#39;), cv2.COLOR_BGR2YCrCb)[:,:,0] #difference in shape: lr*3 and hr, due to sub-sampling (few pixels difference) #--&gt; reshape lr lr_img = lr_img[:-1,:-1] hr_img = hr_img[:lr_img.shape[0]*r,:lr_img.shape[1]*r] return lr_img, hr_img def colored_prediction(self, img_n): &#39;creates colored prediction&#39; #get corresponding images filename = self.dir + img_n hr_img = Image.open(filename + &#39;.png&#39;).convert(&#39;YCbCr&#39;) lr_img = Image.open(filename[:-9] + &#39;-lowRes.png&#39;).convert(&#39;YCbCr&#39;) #upsample bicubic = lr_img.resize(hr_img.size, Image.BICUBIC) return lr_img, bicubic, hr_img # return as pil images def getSlideList(slide_subfolders, path): slides = [] for i, slide in enumerate(slide_subfolders): slides.append(Slide(path, slide, i)) return slides slides = getSlideList(slide_subfolders, path) . PSNR calculation . def psnr_from_mselist(mse_list): mse = np.mean(mse_list) if mse == 0: return float(&#39;inf&#39;) else: return 20*np.log10(255/np.sqrt(mse)) def calc_mse(img_pred, img_hr): return np.mean((img_pred*255 - img_hr*255)**2) . Obtaining low resolution images . # ## DONT RUN THIS CELL, LR IMAGES ARE ALREADY MADE AND PRESENT IN DIRECTORY # def createLowRes(img_name, dir_91): # &#39;saves low resolution image&#39; # # Call HR image # filename = dir_91 + img_name + &#39;.png&#39; # hr_img = cv2.cvtColor(cv2.imread(filename), cv2.COLOR_BGR2RGB) # # Blur HR image # blur_img = cv2.GaussianBlur(hr_img,(5,5),0) # # Apply subsampling # lr_img = blur_img[::r,::r] # #print(&#39;Shape HR image: &#39;, hr_img.shape) # #print(&#39;Shape LR image: &#39;, lr_img.shape) # # Save lr_img # im = Image.fromarray(lr_img) # im.save(dir_91 + img_name + &#39;_lr.png&#39;) # for i in range(len(slides[0].)): # dir_91 = path + slide_subfolders[0] # createLowRes(slides[0].namelist[i], dir_91) . . Training . -loss -optimizer . -stopping criterion (anders gedaan dan in paper) -learning rate verhaal . Eigenlijk zouden we hier nog validation bij moeten doen. Dus ongeveer 80% van patches gebruiken voor training, 20% gebruiken voor validation. Maar dat heb ik nu niet toegevoegd. Zouden we nog kunnen doen na morgenochtend . Hyperparameter settings . ## Loss and optimizer criterion = nn.MSELoss() optimizer = torch.optim.Adam(ConvNet.parameters(), lr=0.01) ## Stopping criterion num_epoch = 5000 #amount of epochs ## Scheduler for dynamic reduction of the learning rate threshold_mu = 1e-6 # Treshold for decreasing learning rate. Default threshold: 0.0001, paper does not say anything about value of threshold. factor_value = 0.8 # Amount of decay per step, new lr = factor_value*lr. Default value: 0.1, paper does not say anything about this value. scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=&#39;min&#39;, factor=factor_value, patience=2, threshold=threshold_mu, min_lr=0.0001, eps=1e-08, verbose=True) ## Batch size batch_size = 16 ## training validation ratio train_val_ratio = 0.95 . . Data loader . -waarom en hoe we data loader/generator hebben gebruikt . class DataGenerator(data.Dataset): &#39;Generates the dataset that is used for training the ESPCN&#39; def __init__(self, slides): self.slides = slides self.transform = torchvision.transforms.Compose([ torchvision.transforms.ToTensor()]) def __len__(self): return len(self.slides.patchlist) def __getitem__(self, idx): lr_patch, hr_patch = self.slides.createPatch(self.slides.patchlist[idx]) # transform images to pytorch tensors lr_patch = self.transform(lr_patch) hr_patch = self.transform(hr_patch) return lr_patch, hr_patch # Put DataGenerator in DataLoader full_dataset = DataGenerator(slides[0]) # Split between training and validation set train_size = int(train_val_ratio * len(full_dataset)) validation_size = len(full_dataset) - train_size training_set, validation_set = torch.utils.data.random_split(full_dataset, [train_size, validation_size]) # num_workers should parallelize loading of the batches by the CPU training_generator = data.DataLoader(training_set, batch_size=batch_size, num_workers=batch_size, shuffle=&#39;True&#39;) validation_generator = data.DataLoader(validation_set, batch_size=1, num_workers=1, shuffle=&#39;False&#39;) . Training the model . # Lists used for saving losses and PSNR loss_list = [] epoch_loss_list = [] val_loss_list = [] validation_psnr = [] # Start stopwatch t0 = time.time() for epoch in range(num_epoch): # Switch to training mode ConvNet.train() for i, (lr_patch, hr_patch) in enumerate(training_generator): # Transfer training data to active device lr_patch, hr_patch = lr_patch.to(device), hr_patch.to(device) # Run the forward pass outputs = ConvNet(lr_patch) loss = criterion(outputs, hr_patch) loss_list.append(loss.item()) # Backprop and perform Adam optimisation optimizer.zero_grad() loss.backward() optimizer.step() # Save loss every epoch epoch_loss = np.sum(loss_list)/len(training_generator) epoch_loss_list.append(epoch_loss) if epoch % 50 == 0: print(&quot;Epoch&quot;, epoch, &quot;loss: {}&quot;.format(epoch_loss)) # Save model every 1000 epochs if epoch % 1000 == 999: &#39;Save model every 1000 epochs (in case Google Colab stops runtime)&#39; model_name = &#39;test3_mu_&#39; + str(threshold_mu) + &#39;_&#39; + str(epoch+1) + &#39;_epochs&#39; path_model = path+&#39;/introductory_notebooks/saved_models/&#39; + model_name torch.save(ConvNet.state_dict(), path_model) print(&#39;Model saved as: &#39;, model_name) # Step to next step of lr-scheduler scheduler.step(epoch_loss) loss_list = [] # Enter validation mode ConvNet.eval() # Keep track of mse for every patch, to collectively calculate PSNR per epoch epoch_mse = [] with torch.no_grad(): for i, (lr_patch, hr_patch) in enumerate(validation_generator): # Transfer training data to active device (GPU) lr_patch, hr_patch = lr_patch.to(device), hr_patch.to(device) # Predict output img_pred = ConvNet(lr_patch) # Calculate validation loss loss = criterion(img_pred, hr_patch) loss_list.append(loss.item()) # Calculate mse for every sample img_pred = img_pred[0].cpu().numpy() hr_patch = hr_patch[0].cpu().numpy() epoch_mse.append(calc_mse(img_pred, hr_patch)) # Calculate validation psnr on the complete epoch from all individual MSE&#39;s val_psnr = psnr_from_mselist(np.array(epoch_mse)) validation_psnr.append(val_psnr) # Save validation loss every epoch val_loss = np.sum(loss_list)/len(validation_generator) val_loss_list.append(val_loss) loss_list = [] print(&#39;Training took {} seconds&#39;.format(time.time() - t0)) print(&#39;Seconds per epoch:&#39;,(time.time()-t0)/num_epoch) . Plot training results . # Show training and validation loss of current model in memory plt.figure(figsize=(8,8)) plt.subplot(2,1,2) plt.title(&#39;PSNR for validation data&#39;) plt.plot(np.arange(num_epoch), validation_psnr) plt.subplot(2,1,1) plt.title(&#39;Loss per epoch&#39;) plt.plot(np.arange(num_epoch), epoch_loss_list, label=&#39;Training loss&#39;) plt.plot(np.arange(num_epoch), val_loss_list, label=&#39;Validation loss&#39;) plt.yscale(&quot;log&quot;) plt.legend() plt.show() . Saving the model . # Save the current model in memory model_name = path + &#39;/introductory_notebooks/mu/&#39; + &#39;mu_1e-2&#39; path_name = model_name + &#39;_validation_psnr&#39; np.save(path_name, validation_psnr) path_name = model_name + &#39;_epoch_loss_list&#39; np.save(path_name, epoch_loss_list) path_name = model_name + &#39;_val_loss_list&#39; np.save(path_name, val_loss_list) path_model = model_name + &#39;_model&#39; torch.save(ConvNet.state_dict(), path_model) print(&#39;Model saved as: &#39;, model_name) . Testing . Loading the model . model_name = &#39;final_test_5000_epochsweights&#39; path_model = path + &#39;/introductory_notebooks/combined/&#39; + model_name ConvNet.load_state_dict(torch.load(path_model)) . Test image functions . def generate_figure(lr_img, sr_img, hr_img): &quot;&quot;&quot;Show the low resolution image, together with the original high resolution image and upscaled version. Is used for a visual representation of how well the model behaves. &quot;&quot;&quot; f = plt.figure(figsize=(8*3,8)) f.add_subplot(1, 3, 1) plt.imshow(transforms.ToPILImage(&#39;YCbCr&#39;)(lr_img[0]).convert(&#39;RGB&#39;)) f.add_subplot(1, 3, 2) plt.imshow(transforms.ToPILImage(&#39;YCbCr&#39;)(sr_img[0]).convert(&#39;RGB&#39;)) f.add_subplot(1, 3, 3) plt.imshow(transforms.ToPILImage(&#39;YCbCr&#39;)(hr_img[0]).convert(&#39;RGB&#39;)) def save_tensor_as_image(lr_img, sr_img, hr_img, i): &quot;&quot;&quot;Save the tensors as PIL images in the saved_image folder.&quot;&quot;&quot; transforms.ToPILImage(&#39;YCbCr&#39;)(lr_img[0]).convert(&#39;RGB&#39;).save(path + &#39;/introductory_notebooks/saved_images/lr_img_&#39; + str(i) + &#39;.png&#39;) transforms.ToPILImage(&#39;YCbCr&#39;)(sr_img[0]).convert(&#39;RGB&#39;).save(path + &#39;/introductory_notebooks/saved_images/sr_img_&#39; + str(i) + &#39;.png&#39;) transforms.ToPILImage(&#39;YCbCr&#39;)(hr_img[0]).convert(&#39;RGB&#39;).save(path + &#39;/introductory_notebooks/saved_images/hr_img_&#39; + str(i) + &#39;.png&#39;) . Data loader . class DataGenerator_test(data.Dataset): &#39;Generates the dataset used for testing&#39; def __init__(self, slides): self.slides = slides self.transform = torchvision.transforms.Compose([ torchvision.transforms.ToTensor()]) def __len__(self): return len(self.slides.namelist) def __getitem__(self, idx): lr_img, bicubic, hr_img = self.slides.colored_prediction(self.slides.namelist[idx]) lr_img = self.transform(lr_img) hr_img = self.transform(hr_img) bicubic = self.transform(bicubic) return lr_img, bicubic, hr_img ## Put DataGenerator in DataLoader def DataGenerator(slides): test_set = DataGenerator_test(slides) test_generator = data.DataLoader(test_set, batch_size=1, shuffle=False) return test_generator . Testing the model . # Create predictions #0: Yang91. #1: Set5 #2: Set14 #3: BSD300 #4: BSD500 #5: SuperText136 #To train on all datasets: for j in range(len(slides[1:])) rnd.seed(4) testsets = [&#39;Set5&#39;,&#39;Set14&#39;,&#39;BSD300&#39;,&#39;BSD500&#39;,&#39;SuperText136&#39;] for j in range(len(slides[1:])): test_generator = DataGenerator(slides[j+1]) show = rnd.randint(0,len(test_generator)) start_time = time.time() outputs = [] mse_list = [] #for epoch in range(1): for i, (lr_img, bicubic, hr_img) in enumerate(test_generator): #currently in form: [1,3,x,y] with torch.no_grad(): # Only test on the Y (intensity channel) to_network = (lr_img[:,0,:,:]).unsqueeze(0) #Run the forward pass. Testing is done on CPU now outputs = ConvNet.cpu()(to_network) img_pred = outputs[0] # substitute the Y channel from bicubic with the one outputted by the model bicubic[:,0,:,:] = img_pred mse = calc_mse(img_pred.numpy(), hr_img.numpy()[:,0,:,:]) mse_list.append(mse.item()) if i == show: generate_figure(lr_img, bicubic, hr_img) save_tensor_as_image(lr_img, bicubic, hr_img, i) elapsed_time = time.time() - start_time print(testsets[j], &#39;: &#39;, psnr_from_mselist(mse_list)) .",
            "url": "https://sfalkena.github.io/ESPCN_reproduction/2020/04/20/ESPCN_reproduction.html",
            "relUrl": "/2020/04/20/ESPCN_reproduction.html",
            "date": " • Apr 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://sfalkena.github.io/ESPCN_reproduction/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://sfalkena.github.io/ESPCN_reproduction/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi! Glad that you want to know more about me! My name is Sieger Falkena, I am 24 years old and currently studying my Masters Embedded Systems. I made this website to show some of the work that I did during my studies. More about me will follow soon :) . Just so you know, this website is powered by fastpages 1. Take a look, its cool :) . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://sfalkena.github.io/ESPCN_reproduction/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://sfalkena.github.io/ESPCN_reproduction/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}